sklearn特征专用的类很多不支持一维数组，用reshape(-1,1)
ravel()降维

正则化回归和随机森林：内置特征选择

深度学习：内置特征提取



# 数据预处理和特征工程
## 概述
### 数据预处理与特征工程
创造特征：常以降维算法的方式实现
### sklearn中的数据预处理与特征工程
**Dimensionality reduction模块**和**Preprocessing模块**
preprocessing几乎包含数据与处理所有内容
impute填补缺失值专用
feature_selection包含特征选择的各种方法的时间
decomposition包含降维算法

《Feature Engineering for Machine Learning：Principes and Techniques for Data Scientists》

## 数据预处理
### 数据无量纲化
以梯度和矩阵为核心的算法中：逻辑回归，SVM，神经网络，无量纲化可以加快求解速度。
距离类模型：K近邻，K-Means聚类，无量纲化可以提升模型精度，避免某一个取值范围特别大的特征造成影响

决策树不需要

#### 中心化和缩放处理
##### 数据归一化：Normalization
减去最小值，除以极差
$$x^*=\frac{x-min(x)}{max(x)-min(x)}$$

注意：正则化regularization，不是数据预处理的手段
```
from sklearn.preprocessing import MinMaxScaler
# MinMaxScaler有一个重要的参数feature_range，默认[0,1]，是我们希望把数据压缩到的范围
```
```
scaler = MinMaxScaler()
# 实例化，可以传入feature_range=[a,b]
scaler = scaler.fit(data)
# fit，这里的本质是生成min和max
# 这里data必须是2维及以上的数组
result = scaler.transform(data)
# 通过接口导出结果
```
```
result_ = scaler.fit_transform(data)
# 训练和导出结果一步达成
```
```
scaler.inverse_transform(result)
# 获得归一化之前的数据
```
```
# 当X中的特征数量非常多的时候，fit会报错，此时用partial_fit作为训练接口
scaler = scaler.partial_fit(data)
```
##### 数据标准化Standardization
减去均值，按标准差缩放，服从0,1正态分布
```
scaler = StandardScaler()
# 实例化
scaler.fit(data)
# fit
x_std = scaler.transform(data)
# 用接口导出结果
```
```
scaler.fit_transform(data)
# 一步达成结果
# 也可以转回去
```
##### 选择
优先标准化，归一化容易受到极值的影响

##### 其他
Standard
MinMax
MaxAbs
Robust
Normalizer
PowerTransformer
QuantileTransformer
KernelCenterer

### 缺失值
```
class sklearn.impute.SimpleImputer(missing_values=nan, strategy='mean', fill_value=None, verbose=0, copy=True)

missing_values：缺失值长什么样，默认np.nan
strategy：填补缺失值的策略。mean：均值，median：中值（前两个只能是数值型），most_frequent：众数，constanct：参考fill_value的值
fill_value：strategy为constant的时候使用
copy：创建特征矩阵的副本
```
这块儿用pandas和numpy做更方便
### 处理分类型特征：编码与哑变量
#### preprocessing.LabelEncoder：标签专用
```
data.iloc[:,-1] = LabelEncoder().fit_transform(data.iloc[:,-1])
```
```.classes_```属性可以看标签中的类别

LabelBinarizer可以对标签做哑变量，许多算法可以处理多标签问题（决策树），但这样的做法在现实中不常见
#### 特征专用的编码器：preprocessing.OrdinalEncoder
```.categories_```可以看每个特征中都有哪些类
#### 独热编码，创建哑变量：OneHotEncoder
##### 分类变量的性质
名义变量：彼此间没关系
有序变量：可比较，但是不能计算
有距变量：可计算
##### 哑变量

|      | 原来的编码 | 哑变量    |
| ---- | ---------- | --------- |
| A    | 0          | [1, 0, 0] |
| B    | 1          | [0, 1, 0] |
| C    | 2          | [0, 0, 1] |

##### 参数、属性、方法
参数categories可以传入一个列表告诉它每列有几类，也可以填'auto'【默认值，这里写是因为不写会报更新红条】

get_feature_names()获取哑变量各列的列名
### 处理连续型特征：二值化与分段
#### preprocessing.Binarizer将数据二值化
#### 分箱preprocessing.KBinsDiscretizer
## 特征选择
理解数据，特征选择完全独立于任何机器学习算法
### Filter过滤法
#### 方差过滤
#### 相关性过滤
### Embedded嵌入法
### Wrapper包装法
### 总结
## 案例

# 降维算法PCA和SVD
## 概述
### 什么叫维度
降维的目的：算法运行的更快、效果更好，数据可视化
### sklearn中的降维算法
decomposition：矩阵分解

主成分分析PCA：有各种PCA
因子分析：FA
独立成分分析
字典学习
高级矩阵分解LatentDirichletAllocation和NMF
其他矩阵分解：稀疏编码
## PCA和SVD
希望能判断信息的量，既减少特征数量，又保留大部分信息

PCA衡量信息量的指标：样本方差
SVD衡量信息量的指标：奇异值
### 降维是怎样实现的
降维算法的新特征没有任何解释性
```
sklearn.decomposition.PCA(n_components=None, copy=True, whiten=Flase, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)
```
### 重要参数n_components
降维后保留的维度，超参数
#### 迷你案例：高维数据的可视化
```
X_dr = PCA(2).fit_transform(X)

# 画图部分：
plt.figure()
for i in [0, 1, 2]:
    plt.scatter(X,Y,alpha=.7,c=colors[i],label=iris.target_names[i])
plt.legend()#给图加图例，就是红色表示什么，蓝色表示什么的那个
plt.title("str")
plt.show()
```

```
pca.explained_variance_每个特征可解释性方差的大小
pca.explained_variance_ratio_每个特征的占比，可解释性方差贡献率
pca.explained_variance_ratio_.sum()看看最后保留了多少信息
```

**累积可解释性方差贡献曲线**：横坐标：保留的特征个数，纵坐标：保留的信息率
```
import numpy as np
pca_line = PCA().fit(X)
plt.plot( [1,2,3,4], np.cumsum(pca_line.explained_variance_ratio) )
plt.xticks([1,2,3,4])#限制坐标轴显示
plt.show()
```
选让曲线忽然变得平滑的点
#### 最大似然估计自选超参数
#### 按信息量占比选超参数
### 重要参数svd_solver
#### PCA中的SVD从哪里来
#### svd_solver与random_state
### 重要接口inverse_transform
#### 神奇的inverse_transfrom
#### 迷你案例
#### 迷你案例
## 专题：从哪里开始机器学习
吴恩达的课
数据挖掘导论
Python数据科学手册
## 案例：PCA在手写数据集
## 案例：PCA在人脸识别
## 附录

# 近邻Nearest Neighbor
KNN：对于待判断的点，找到离它最近的几个数据点，根据他们的类型决定待判断点的类型

特点：完全跟着数据走，没有数学模型可言

适用场景：
需要一个特别容易解释的模型的时候
# 贝叶斯Bayesian
Naive Bayes：根据条件概率计算判断点的类型

适用场景：
需要一个比较容易解释，而且不同维度之间相关性较小的模型
可以高效处理高维数据，但结果可能不太好

# 决策树Decision tree
沿着特征做切分

适用场景：
希望更好的理解数据的时候
# 随机森林Random forest#####
大量决策树的集成

适用场景：
数据维度相对较低（几十），同时对准确性有较高要求时。
# SVM，support vector machine#####
找到不同类别的分界面
https://www.bilibili.com/video/BV1Hs411w7ci?p=1
SVM有三宝：间隔、对偶、核技巧

核技巧：与SVM并没有对应关系，之前就已经被广泛研究，可以让SVM映射到高维空间，实现非线性分类



SVM分类：

hard-margin SVM

soft-margin SVM

kernel SVM

## hard-margin SVM

SVM本质上是一个判别模型，与概率没有关系

hard-margin SVM又称最大间隔分类器

推导：
$$
样本点x_i∈R^p,标签y_i∈\{-1,+1\}
$$
要满足：
$$
w^Tx_i+b>0,y_i=+1 \\
w^Tx_i+b<0,y_i=-1
$$
即
$$
y_i(w^Tx_i+b)>0
$$
margin的定义：离超平面最近的点到超平面的距离
$$
margin(w,b)=min\ distance(w,b,x_i)\\
=\min\limits_{x_i}\ \frac{1}{||w||_2}|w^Tx_i+b|
$$
要最大化margin：
$$
\max\limits_{w,b}margin(w,b)
$$

离超平面最近的点叫做支持向量，那么支持向量的$$w^Tx+b$$必然等于+1或-1  
那么之前的式子就可以转换为：
$$
\min\limits_{w,b}\frac12w^Tw\\
s.t.\ \ \ \ \ y_i(w^Tx_i+b)≥1
$$

转换为了一个优化问题，并且是凸优化问题

### 对偶
原优化问题在理想情况很美妙，但是有时候，w维度特别高，N特别大，或者在有的问题里维度可能无限（参数经过非线性转化），所以引出了对偶问题  
我们把原来的优化问题称为原问题  

拉格朗日函数、拉格朗日乘子法：求极值

原问题：
$$
\min\limits_{w,b}\frac12w^Tw\\
s.t.\ y_i(w^Tx_i+b)≥1
$$
拉格朗日函数：
$$
L(w,b,λ) = \frac12w^Tw+\sum_{i=1}^Nλ_i(1-y_i(w^Tx_i+b))
$$
原问题转换为无约束问题：（下面这个依旧是原问题）
$$
\min\limits_{w,b}\max\limits_λL(w,b,λ) \\
s.t.\ λ_i≥0
$$
对偶问题为：（强对偶）
$$
\max\limits_λ\min\limits_{w,b}L(w,b,λ) \\
s.t.\ λ_i≥0
$$
继续在一堆有点看不懂的推导后，反正是算出了w和b的表达式：  
可以看出，w、b都主要是data的线性组合，而且是支持向量的线性组合

## soft-margin SVM
实际问题可能会有噪声，或者不可分的，那么硬间隔就做不出来  
soft的意思是允许一点点错误  
即：
$$
min\frac12w^Tw + loss
$$

loss的想法：
1. 用不满足约束的点数
有问题，这样的函数是跳跃的，没法求导
2. 用距离：hinge loss 因为图像像合页

## sklearn
https://www.bilibili.com/watchlater/#/BV19t411a7j1/p1
2个线性类：LinearSVC和LinearSVR（线性支持向量分类、线性支持向量回归）
非线性多维2个：SVC和SVR
Nu支持向量：NuSVC、NuSVR；Nu能规定支持向量的个数
无监督异常值检测：OneClassSVM

直接使用libsvm的函数：提供了底层计算和参数选择的库
交叉验证、预测边际

### Kernel的选择

| 输入    | 含义       | 解决问题 | 核函数的表达式 | 参数gamma | 参数degree | 参数ceof0 |
| ------- | ---------- | -------- | -------------- | --------- | ---------- | --------- |
| linear  | 线性核     | 线性     |                | NO        | NO         | NO        |
| poly    | 多项式核   | 偏线性   |                | YES       | YES        | YES       |
| sigmoid | 双曲正切核 | 非线性   |                | YES       | NO         | YES       |
| rbf     | 高斯径向基 | 偏非线性 |                | YES       | NO         | NO        |

月亮型、环形、线性噪音：rbf

混杂，都不行

linear和poly都比较接近

sigmoid一直在瞎搞

poly在图像处理里很常用，poly中d=1就是线性的


### 看样本分布
可以拿PCA降维成2维看一看
### 样本不均衡问题

### 先拿各个核函数跑一下
cache_size，默认200，需要调大，单位MB，允许使用的内存
poly很可能跑不出来

### 特征处理
存在的问题：
1. 数据的量纲不统一
2. 数据的分布是偏态的
处理：
标准化

# 逻辑斯特回归Logistic regression
LR，拟合一个概率学中的函数

适用场景：
本质上是一个线性分类器，所以处理不好特征之间的相关关系。解释性好
# 判别分析Discriminant analysis
LDA，线性判别分析，高维投影后进行分类

适用场景：
高位数据需要降维，准确率不高，假定样本正态
# 神经网络Neural Network
准确率依赖于庞大的训练集

适用场景：
数据量庞大，参数之间存在联系
# Rule-based methods
C5.0 Rules，决策树的变种

适用场景：
少见，准确度比决策树稍低，提供明确小规则来解释结果
# 提升算法Boosting######
AdaBoost，集成

适用场景：
准确度还行，自带特征选择

# AdaBoost

## 集成学习概述
### 集成学习算法定义
### bagging（装袋）
自主聚集
根据均匀概率分布从数据集中重复抽样。有放回随机抽样，新数据集和原始数据集大小相等。

代表算法：随机森林
### boosting（提升）
迭代的过程，自适应改变训练样本的分布。给每个样本一个权重，每轮训练后调整。使得弱分类器聚焦到难以分类的样本上。

弱分类器间是有联系的

代表算法：AdaBoost（提升树）、GBDT（梯度提升树）、XGBoost
### 结合策略
#### 平均法
回归预测常用

算术平均与加权平均
#### 投票法
相对多数投票法：少数服从多数

绝对多数投票法：相对的基础上，要求票数过半，不然拒绝预测

加权投票法：加权平均法
#### 学习法
代表算法：stacking

让结果进行输入，再训练

## Adaboost算法
adaptive boosting 自适应boosting
### 计算样本权重
1/n
### 计算错误率
c=分错的数量/样本总数【c应该是epsilon
### 计算弱分类器权重
α=0.5ln((1-c)/c)
### 调整权重值
分对的样本权重降低
$$
D^{(t+1)}=\frac{D^te^(-a)}{Sum(D)}
$$
分错的增加
$$
D^{(t+1)}=\frac{D^te^(a)}{Sum(D)}
$$
## 基于单层决策树构建弱分类器
### 构建简单数据集
### 构建单层决策树
## 完整AdaBoost算法的实现
## 基于AdaBoost的分类
## 案例：在病马数据集上应用AdaBoost
### 导入数据及
### 构建分类函数

# 装袋算法Bagging
集成算法，一定程度上可以避免过拟合

适用场景：
效果依赖于调参
# Stacking
多个分类器的结果上再套一个分类器

适用场景：
依赖调参

# 多专家模型Mixture of Experts
合并神经网络的分类结果
# 文献
Do we Need Hundreds of Classifiers to Solve Real World Clssification Problems
比较子不同数据及上，不同分类器的实际效果，121-179