本文根据以下视频整理。
https://www.bilibili.com/video/av6538245?from=search&seid=436175425155932048
B站 陈渝老师 清华大学


# 1 操作系统的概述

## 1.1 操作系统的定义

**用户角度：**操作系统是一个控制软件

- 管理应用程序
- 为应用程序提供服务
- 杀死应用程序
- 资源管理
- 管理外设/分配资源

## 1.2 操作系统对物理资源的抽象

将**CPU**抽象成**进程**。
将**磁盘**抽象成**文件**。
将**内存**抽象成**地址空间**。

## 1.3 操作系统的架构层次：

硬件之上，应用软件之下。
（为应用软件提供服务支持）。

## 1.4 shell与kernel

Linux，Windows界面属于外壳shell（与User交互），而不是内核kernel，而kernel是我们的研究重点，在shell之下。

## 1.5 Kernel的内部组件

+ CPU调度器
+ 物理内存管理
+ 虚拟内存管理
+ 文件系统管理
+ 中断处理和IO设备驱动 （底层硬件）

## 1.6 OS Kernel的特征

+ 并发
+ 共享
+ 虚拟
+ 异步

### 1.6.1 并发

并发：指一段时间内执行多个任务
【并行：一个时间点上多个程序运行，要求多个CPU】

计算机系统中同时存在多个运行的程序，需要OS管理和调度

### 1.6.2 共享

+ 同时访问
+ 互斥共享

### 1.6.3 虚拟

利用多道程序设计技术，让每一个用户都觉得有一个计算机专门为他服务。【如时分复用技术】

### 1.6.4 异步

+ 程序的执行不是一步到底的，而是走走停停，向前推进的速度不可预知。
+ 但只要运行环境相同，OS要保证程序运行的结果也相同

## 1.7 操作系统的结构

1. 简单的操作系统
MS-DOS：不分模块的单体内核 （内部通过函数调用访问，缺点：复杂，紧耦合，易受攻击）

2. 微内核
尽可能把内核功能移植到用户空间，缺点：性能低。

3. 分为外核，内核，外核负责和硬件打交道，内核和应用打交道。

4. 虚拟机
在操作系统和硬件之间。虚拟出一套完整的硬件。
VMs（虚拟机）->VMM（虚拟机监视器）->物理机硬件，多操作系统共享硬件资源。

# 2 操作系统基础操作

## 2.1 操作系统的启动

计算机最基本的有3部分：CPU、I/O、内存。
通过总线相连。

OS一开始是位于磁盘中的，DISK。

BIOS：基本I/O处理系统（ basic I/O system）。
基本功能：通电后检测外设。

Bootloader：一个小程序，用于加载OS到内存中。

### 2.1.1 通电后BIOS的操作

当电脑通电时，BIOS开始执行，开始执行的地址是固定的，段寄存器CS和指令寄存器IP能够确定一个内存地址，例如CS:IP = 0xf000:fff0.（x86）

1. POST（加电自检），寻找显卡和执行BIOS。（显示器，键盘…是否正常）。

2. 将bootloader从磁盘放到内存中去。
bootloader存放在磁盘的位置也是固定的，一般放在磁盘的**第一个**主引导扇区。磁盘的引导扇区（512字节）加载到0x7c00；跳转到CS:IP=0000:7c00的内存区域（以便下一步）

3. Bootloader将操作系统的代码和数据从硬盘加载到内存中；跳转到操作系统的起始地址。OS在磁盘中的位置也是由BIOS设定好的。

## 2.2 中断、异常和系统调用

### 2.2.1 操作系统与设备和程序的交互方式

+ 面向外设：中断
+ 面向程序：系统调用、异常

### 2.2.2 三者的定义

系统调用（来源于应用程序）：
应用程序主动向操作系统发出服务请求。

异常（来源于不良的应用程序）：
非法指令或其它花的处理状态（e.g.内存出错）。

中断（来源于外设）：
来自不同的硬件设备的计时器和网络的中断。

### 2.2.3 为什么应用程序不能直接访问硬件而是通过操作系统？

- 计算机运行时，内核是被信任的第三方。
- 只有内核可以执行特权指令。
- 为了方便应用程序。

### 2.2.4 三者的区别和特点

#### 2.2.4.1 产生的源头

+ 中断：外设（键盘/鼠标/网卡/声卡/显卡，可以产生各种事件）

+ 异常：应用程序意想不到的行为（e.g.异常，恶意程序，应用程序需要的资源未得到满足）

+ 系统调用（system call）：应用程序请求操作提供服务（e.g.打开/关闭/读写文件，发送网络包）

#### 2.2.4.2 处理时间

+ 中断：异步；
+ 异常：同步；
+ 系统调用：同步或异步。

异步：一种通讯方式，不知道什么时候会产生消息，消息产生了再去通知。
同步：明确知道什么时候会产生消息。

+ 中断：异步
事件产生时，应用程序并不知道它什么时候会产生，只有当他产生了才去处理

+ 异常：同步
一定是执行到某一条特定指令时一定会产生

+ 系统调用：同步或异步。
产生系统调用是同步的：一定是执行到某一条特定指令时一定会产生。
从系统调用的返回时间来说：如果应用程序发出系统调用请求后一直在等待则为同步、如果应用程序去做别的事了就是异步。

#### 2.2.4.3 响应

+ 中断：持续，对用户应用程序是透明的
+ 异常：杀死或者重新执行产生异常的应用程序指令
+ 系统调用：等待和持续

持续：继续执行。

### 2.3 中断和异常处理机制

中断是外设的事件，异常是CPU的事件；中断/异常迫使CPU访问一些被中断和异常服务访问的功能。（这句话读不懂，PPT上是这么写的）

这个过程中，既有硬件的处理，也有软件的处理。

要知道中断是什么中断，维护一个中断表，key是中断号，value是中断服务的地址。

当发生中断，就可以根据中断表，跳到中断服务的地址开始执行。在这个过程中，还需要保存一下现场，使得中断结束后可以继续回来接着执行。

#### 2.3.1 中断处理机制

##### 2.3.1.1 硬件部分

外设产生一个中断标记，CPU看到中断标记后，可以产生一个中断号，把中断号发给OS，让OS处理。

+ 产生中断标记
+ 生成中断号

##### 2.3.1.1 软件部分

操作系统先保存被打断的执行现场。根据中断号，找到中断服务程序地址，跳到中断服务程序执行。处理完之后，恢复之前的执行现场。

+ 保存当前处理状态
+ 中断服务程序处理
+ 清除中断标记
+ 恢复之前保存的处理状态

#### 2.3.2 异常处理机制

异常：异常编号
+ 保存现场
+ 异常处理：杀死产生异常的程序；或 重新执行异常指令
+ 恢复现场

### 2.4 系统调用

程序需要操作系统做一些事情，发送一个请求，这个请求叫做系统调用

+ 一条指令会触发一个系统调用

+ 程序访问主要是通过高层次的API接口而不是直接进行系统调用。

#### 2.4.1 内核态和用户态

+ 用户态：应用程序在执行的过程中，CPU执行的特权级的状态（很低，不能访问特殊机器指令和IO）。

+ 内核态：应用程序在执行的过程中，CPU执行的特权级的状态（高，操作系统可以执行CPU任何一条指令）。

+ 系统调用时涉及到特权级从用户态到内核态的转换

+ 应用程序和操作系统有各自的堆栈，这两个变化比函数调用的开销更大（要切换特权级和堆栈），但更安全和可靠。（而程序调用是在一个栈空间实现参数的调用和返回）。

#### 2.4.2 操作系统如何实现系统调用

+ 通常情况下，存在与每个系统调用相关的序号，系统调用接口根据这些序号来维护表的索引。

+ 系统调用接口调用内核态中预期的系统调用，并返回系统调用的状态和其它任何返回值。

+ 用户不需要知道系统调用是如何实现的，只需要获取API和了解操作新系统将什么作为返回结果。操作系统接口的细节大部分都隐藏在API中，并通过运行程序支持的库来管理。

### 2.5 跨越操作系统边界的开销

系统调用、异常、中断的处理，是跨越了操作系统的边界的（操作系统和应用程序的边界，操作系统和外设的边界）。这种跨越是有代价的，来保证系统的安全可靠。

+ 开销包括：
   + 建立中断/异常/系统调用号与对应服务例程映射关系的初始化开销；
   + 建立内核堆栈（操作系统和应用程序的堆栈不一样）；
   + 验证参数（操作系统会检查数据）；
   + 内核态映射到用户态的地址空间，更新页面映射权限（内存拷贝开销）；
   + 内核态独立地址空间TLB。

# 3 物理内存管理

## 3.1 连续式内存分配

### 3.1.1 计算机的体系结构

CPU、内存、I/O（外设）

### 3.1.2 内存的层次结构

寄存器

缓存cache

主存（物理内存）

磁盘

寄存器和cache位于CPU中，从上到下，速度越来越慢，价格越来越便宜。

操作系统的任务：做到内存**既快又大**

### 3.1.3 操作系统在内存管理要完成的目标

+ 抽象：逻辑地址空间
希望应用程序不需要考虑内存的底层细节，只需要访问连续的地址空间

+ 保护：独立地址空间
多个程序的地址空间隔离开

+ 共享：访问相同内存
进程间的数据传递

+ 虚拟化：更多的地址空间
内存不够时，暂时不需要访问的数据，放在磁盘上

### 3.1.4 操作系统实现内存管理目标的手段

+ 程序重定位
+ 分段
+ 分页
+ 虚拟内存
+ 按需分页虚拟内存

具体内容将在之后介绍，在这个过程中高度依赖于硬件，一起完成内存管理。必须知道内存的架构，MMU（内存管理单元）。

### 3.1.5 地址空间与地址生成

#### 3.1.5.1 地址空间的定义

+ 物理地址空间：硬件支持的地址空间
+ 逻辑地址空间：一个运行的程序所拥有的内存范围

#### 3.1.5.2 地址空间的生成

生成逻辑地址：经过一系列过程，不需要OS帮忙，编译器等一系列软件完成。

逻辑地址（LA）与物理地址（PA）的对应：CPU中的MMU（内存管理单元），保存了映射关系。

映射关系由OS来生成。

#### 3.1.5.3 地址安全检查

维护一个表，保存程序的内存可访问界限，每次访问的时候都要进行检查

## 3.2 连续内存分配：内存碎片与分区的动态分配

### 3.2.1 内存碎片问题

+ 空闲内存不能被利用的情况

外部碎片：在分配单元间的无法使用内存
内部碎片：在分配单元中的无法使用内存

希望尽量避免

### 3.2.2 分区的动态分配

#### 3.2.2.1 简单的内存管理方法

- 当一个程序准许运行在内存中时，分配一个连续的区间

- 分配一个连续的内存区间给运行的程序以访问数据

#### 3.2.2.2 分区的动态分配策略

##### 3.2.2.2.1 首次适配

现在想分配n个字节，从低地址开始找，碰到的第一个空间比n大的空闲块就使用它。

需求：
1. 需要存在一个按地址排序的空闲块列表
2. 分配需要找一个合适的分区
3. 重分配需要检查，看看自由分区能不能与相邻的空闲分区合并形成更大的空闲块，如果有合并。

+ 优点
   + 简单
   + 易于产生更大的空闲块，向着地址空间的结尾：找到了就分配了，不会破坏后方的大块。

+ 缺点
   + **容易产生外部碎片**
   + 不确定性

##### 3.2.2.2.2 最佳适配

找一个最适合的空闲块，比需求大，且差值最小。

+ 目的：**避免分割大的空闲块；最小化外部碎片产生的尺寸。**
+ 需求：
1. 按尺寸排列的空闲列表
2. 分配需要寻找一个合适的分区
3. 重分配需要搜索和合并于相邻的空闲分区，若有

+ 优点：
   + 大部分分配是小尺寸时很有效；
   + 简单
+ 缺点：
   + 外部碎片；
   + 重分配慢；
   + **易产生很多没用的微小碎片。**

##### 3.2.2.2.3 最差适配

找到一个空闲块，与需求差距最大。

+ 目的：**避免太多的微小碎片**
+ 需求：
1. 按尺寸排列的空闲列表
2. 分配很快(获得最大的分区)
3. 重分配需要合并于相邻的空闲分区，若有，然后调整空闲块列表

+ 优点：
   + 假如分配时是中等尺寸效果最好
+ 缺点：
   + 重分配慢；
   + 外部碎片；
   + **易于破碎大的空闲块以致大分区不能被分配**


以上算法都各有优缺点，这些都只是一些简单的分配算法，在扩展课中会讲解更复杂的分配算法。

### 3.2.3 碎片整理

#### 3.2.3.1 压缩式碎片整理(紧致)

+ 重置程序以合并孔洞
+ 要求所有程序是动态可重置的
+ 问题：何时重置；开销。

#### 3.2.3.2 交换式碎片整理

把暂时不用的内容挪到磁盘里

+ 运行程序需要更多的内存
+ 抢占等待的程序 & 回收它们的内存

## 3.2 非连续式内存分配

### 3.2.1 为什么要非连续内存分配？

+ 连续内存分配的缺点：
1. 分配给一个程序的物理内存是连续的
2. 内存利用率低
3. 有外碎片/内碎片问题
主要是碎片问题

+ 非连续内存分配的优点
1. 分配给一个程序的物理内存是非连续的
2. 更好的内存利用和管理
3. 允许共享代码和数据(共享库等)
4. 支持动态加载和动态链接
主要解决了碎片问题、内存隔离管理方便。


+ 非连续内存分配的缺点
如何建立虚拟地址和物理地址之间的转换
   软件方案（开销太大，考虑结合硬件）
   硬件方案

+ 两种硬件方案
   + 分段Segmentation
   + 分页Paging

### 3.2.2 分段

两个问题：
1. 程序的分段地址空间（分段情况下如何去对地址寻址）
2. 分段寻址方案（如何实现这种机制）

根据应用程序的特点：本身就是一段一段的，如堆、运行栈、数据、调用的函数库。对其更好的分离和共享。

通过映射，使得：
+ 逻辑地址空间是连续的，物理地址是离散的。

分段寻址的方案
+ 要用硬件来支持，提高效率。

#### 3.2.2.1 段访问机制

一个段指一个“内存块”，是一个逻辑地址空间。

段访问机制：通过段号+偏移来实现。 一个二维的二元组(s段号，addr端内偏移)

两种实现方案：
1. 段寄存器+地址寄存器
2. 单地址实现（前端是s，后端是addr）

硬件实现方案：
维护一个段表，哪个段是在哪个内存里的。然后根据偏移找到地址。

分段目前用的比较少。

### 3.2.3 分页

分页和分段类似，是：页+偏移。区别在于页和段的尺寸。段的尺寸可变，页的尺寸不可变。

+ 页/页帧：
划分物理内存至固定大小的帧
   大小是2的幂，e.g.512，4096，8192
划分逻辑地址空间至相同大小的页
   大小是2的幂，e.g.512，4096，8192

页page是逻辑页，页帧frame是物理页。

+ 建立方案：转换逻辑地址为物理地址(pages to frames)
   页表Page Table
   MMU/TLB

#### 3.2.3.1 页帧Frame

物理内存被分割为大小相等的帧

由两部分组成：帧号f和帧内偏移o

f（frame number）：帧号。它是F位的，意味着一共 $$ 2^F $$ 个帧
o（offset）：帧内偏移。它是S位的，因此意味着每帧有 $$ 2^S $$ 字节
$$
物理地址=2^S x f + o。
$$

+ 实例：
16-bit的地址空间，9-bit（512byte）大小的页帧。
那么（3,6）表示的地址就是1542。
S=9，F=16-9=7，f=3，o=6。

#### 3.2.3.2 页page

一个程序的逻辑地址空间被划分为大小相等的页

页内偏移量=帧内偏移量【一个页的大小和一个帧的大小相同】
页号可能不等于帧号。页的总数一般要多与帧的总数，逻辑空间大于物理空间。
如果访问不存在的帧，产生一个内存访问异常。操作系统进一步处理。

用（p，o）表示。

页表：通过页号找到帧号。由OS建立。
通过帧号和offset找到物理地址。

+ 页映射到帧
+ 页是连续的虚拟内存
+ 帧是非连续的物理内存
+ 不是所有的页都有对应的帧

##### 3.2.3.3 页表

页表是一个大数组。索引是页号，内容是帧号。

每一个运行的程序都有一个页表。
   + 属于程序运行状态，会动态变化
   + PTBR：页表基址寄存器（用来找到页表的）

页表里的每一项里，除了帧号，还有一些标志位flags：如该页是否读写，该页是否存在对应的帧。

##### 3.2.3.4 分页机制的性能问题

空间代价和时间代价。
    
+ 页表可能会非常大
如，64位机器，如果一页是1024KB，那么页表是多大？$$2^54$$，明显CPU装不下。一个程序一个页表，n个程序n个页表，就更大了。

+ 页表很大的话访问效率
CPU装不下，只能装在内存里；如果这样，需要访问2次内存，一次访问页表，一次访问程序。

+ 解决办法
   + 缓存caching【时间】
   + 间接访问indirecion【空间】

##### 3.2.3.5 解决时间问题：TLB快表

MMU（内存管理单元）里面有一个TLB（Translation Lock-aside Buffer），是一个缓存，cache。

TLB的key是p，页，value是f，帧。TLB就是对常用页表的一个缓存。
TLB本身使用相关存储器实现的，可以并发快速的查询，容量比较小。

CPU访问的时候，先查TLB。
TLB访问不到的情况，叫TLB miss，只能查页表。并更新TLB。

TLB miss出现的概率：32位系统一个页有4k，要大约访问4k次才可能出现TLB缺失。同时，写程序的时候，也应尽量使得程序有局部性。

TLB miss后，查页表再存的过程。不同CPU的处理是不一样的，x86完全由硬件完成，不需要OS；有的CPU则是由OS完成的。

##### 3.2.3.6 解决空间问题：多级页表

多级页表就是一个树状结构，通过时间来换空间。时间问题则通过TLB缓解。

将页号分为多个部分p1，p2，...。
p1存放的是二级页表的起始地址，用来定位某个二级页表；二级页表里存的则是三级页表的其实地址……一直到最后一级页表，存放的才是帧号。

多级页表还节约了，不对应帧的部分的页的内存。如果有一部分页不对应帧，那么在一级页表中就可以在驻留位中标识出来，不需要建立相应的二级页表。

### 3.2.3.7 反向页表

用帧来作为key，页作为value。可以有效减少表的大小。即节约了多余的逻辑地址的空间，而且整个系统只需要一个表，传统的一个程序就需要一个。

+ 实现思路：
1. 关联内存：关联内存可以快速并发查找。但是问题在于实现复杂，关联内存太小了。
2. hash：用PID与帧号一同作为输入，设计一个hash函数。问题：可能会哈希碰撞（已用PID缓解），哈希计算要对内存取数。【一些高端CPU里采取了这种方式】

# 4 虚拟内存

## 4.1 起因

内存越来越不够用了。CPU的发展远快于内存的发展。
理想的存储器：*更大*、*更快*、*更便宜*，*非易失性*存储。

实际的存储器：存储器层次结构。寄存器--cache--内存--磁盘--磁带

目前可以做到，在OS的管理下，做到一个“更大、更快、更便宜”的内存。通过把不常用的部分放在硬盘上。

## 4.2 技术的发展

+ 如果程序太大，超过了内存的容量，可以采用**手动的覆盖(overlay) 技术**，只把需要的指令和数据保存在内存中
+ 如果是程序太多，超过了内存的容量，可以采用**自动的交换(swapping) **技术，把暂时不能执行的程序送到外存中
->如果想在有限容量的内存中，以更小的页粒度为单位装入更多更大的程序，可以采用**自动的虚拟存储技术**。

### 4.2.1 覆盖技术

目标：在较小的可用内存中运行较大的程序。

做法：把程序按照逻辑分为多个互相独立的程序模块。不会同时执行的模块共享内存，按时间顺序先后来运行。

缺点：
+ 由程序员来把一个大的程序划分为若干个小的功能模块，并确定各个模块之间的覆盖关系，费时费力，增加了编程的复杂度。
+ 覆盖模块从外存装入内存，是以时间换空间。

### 4.2.2 交换技术

让操作系统来管理，把暂时不运行的程序送到外存，从而获得空闲内存空间。

OS把**一个进程的整个地址空间**的内容保存到外存中（换出swap out），而将将外存中的某个进程的地址空间读入到内存中（换入swap in）。

换入换出内容大小为整个程序的地址空间。

+ 问题：
1. 交换时机的确定：只有当内存空间不够或有不够的危险时换出。应该少。【什么时候交换？】
2. 交换区的大小：必须足够大以存放所有用户进程的所有内存映像的拷贝，必须能对这些内存映像进行直接存取。【交换区多大？】
3. 程序换入时的重定位：因为换出换入后的内存位置不一定相同，所以最好采用动态地址映射的方法。【换回来的时候，位置不一样了，寻址怎么办？】

### 4.2.3 覆盖与交换技术的比较

1. 覆盖只能发生在那些（程序内）相互之间没有调用关系的程序模块之间，因此**程序员**必须给出程序内的各个模块之间的逻辑覆盖结构。
2. 交换技术是以在内存中的程序大小为单位来进行的，它不需要程序员给出各个模块之间的逻辑覆盖结构。
3. 交换发生在内存中程序与管理程序或操作系统之间，而覆盖则发生在运行程序的内部。

### 4.2.4 虚拟内存管理技术

覆盖技术的主要问题：程序员负担太大。
交换技术的主要问题：粒度太大。
所以提出了虚存技术：OS完成，小粒度交换。

## 4.3 虚存技术

### 4.3.1 目标

1. 像覆盖技术一样，不是把程序的所有内容都放在内存中，因而能够运行比当前的空闲内存空间还要大的程序。但做得更好，能由**操作系统自动完成**，无需程序员介入
2. 能像交换技术那样，能够实现进程在内存和外存之间的交换，因而获得更多的空闲内存空间。但能做得更好，只对**进程的部分内容**在内存和外存之间进行交换。

### 4.3.2 程序的局部性原理（principle of locality）

指程序在执行过程中的一个较短时间，所执行的指令地址和指令的操作数地址分别局限于一定区域，表现为：

+ 时间局部性：一条指令的一次执行和下次执行，一个数据的一次访问和下次访问都集中在一个较短的时间里。

+ 空间局部性：当前指令和领近的几条指令，当前访问的数据和领近的几个数据都集中在一个较小区域内。

程序的局部性原理表明，从理论上来说，虚拟存储技术是能够实现的，而且在实现了以后应该能够取得一个满意的效果的。

编写程序的时候需要注意，写出拥有局部性的程序。

### 4.3.3 虚存技术——基本概念

在分段或分页内存管理的基础上实现。

+ 在装入程序时候，不需要全部装入内存，只需要当前需要的部分页或段装入内存，让程序开始执行
+ 执行过程中，发现需要的数据不再内存里，产生缺页/段异常，如果有空闲内存，OS把这些数据放入内存中
+ 如果内存满了，则由OS把暂时不会使用的页/段调出，保存在外存上

### 4.3.4 虚存技术——基本特征

+ 空间很大：内存+磁盘
+ 部分交换：只交换部分页/段
+ 不连续性：物理内存分配不连续，虚拟地址空间使用也是不连续的

### 4.3.5 虚拟页式内存管理

#### 4.3.5.1 基本思路

+ 大部分虚拟存储系统都采用虚拟页式存储管理技术。即在页式存储管理的基础上，增加请求调页和页面置换功能。
+ 基本思路：
   一个程序要运行的时候，只装入部分页面。
   发现要访问的数据不在内存中，则向OS发出缺页中断请求，叫请求调页。
   当内存满了的时候，需要把一些页换出去，一些页换进来，叫页面置换功能。

#### 4.3.5.2 页表项中的辅助位

为了实现虚拟页式内存管理，对页表的页表项，增加几个位来辅助。
+ 驻留位：这个页在内存还是外存。1表示：在内存。0表示：不在内存。
+ 保护位：表示只读、可读写、可执行。
+ 修改位：这个页是否被写过。换出的时候是否需要导回到硬盘。
+ 访问页：访问过置1，用于页面置换算法。

#### 4.3.5.3 缺页中断处理过程

如果内存有空闲，则分配一物理页帧，将硬盘中的数据读入内存，修改页表。重新执行指令。

如果没有空闲，则采用页面置换算法，换出去一页（修改过还需要写回），则内存有空闲了，和之前一页。

### 4.3.6 后备存储

后备存储backing store，也叫二级存储。在这里就是磁盘。
在这里有4种：
1. 文件数据
2. 代码段
3. 共享的库程序
4. 在交换过程中产生的一些动态数据，叫swap file，保存在swap空间里

### 4.3.7 虚拟内存性能

有效存储器访问时间：effective memory access time
$$
EAT = 访存时间 * 页面命中几率 + page fault处理时间 * page fault几率
$$
page fault几率：p（页面没有命中）
dirty page几率：q（有写操作）
$$
EAT = 访存时间*(1-p) + 磁盘访问时间*p*(1+q)
$$

## 4.4 页面置换算法

页面置换算法大体可以分为两种：
1. 局部置换算法：最优页面置换算法、FIFO、LRU、Clock、Enhance Clock、LFU
2. 全局置换算法：工作集置换算法、缺页率页面置换算法




### 4.4.1 功能目标

+ 功能：
   当缺页中断发生，需要调入新的页面而内存已满时，选择内存当中哪个物理页面被置换。

+ 目标：
   尽可能减少页面的换入换出次数(即缺页中断的次数)。把未来不再使用的或短期内较少使用的页面换出，通常只能在局部性原理的指导下依据过去的统计数据来进行预测。

+ 页面锁定(frame locking)：
用于描述必须常驻内存的操作系统的关键部分或时间关键(time-critical)的应用进程。实现方法是，在页表中添加锁定标志位(lock bit)。

### 4.4.2 最优页面置换算法

+ 基本思路：
当一个缺页中断发生时，对于保存在内存中的每一个逻辑页面，计算在它的下一次访问之前，还需要等待多长时间，从中选择等待时间最长的那个，作为被置换的页面。

+ 不过，这只是一种理想情况，在实际中无法实现，因为操作系统无法知道每一个页面要等待多长时间以后才会被再次访问。

+ 可用作其它算法的性能评价的依据(在一个模拟器上运行某个程序，并记录每一次的页面访问情况，在第二遍运行时即可使用最优算法)。

### 4.4.3 先进先出算法

First In First Out， FIFO
选择在内存中驻留时间最长的页面淘汰。

+ 性能较差，并且有Belady现象（随着页总数的增加，调页次数反而增加）

### 4.4.4 最近最久未使用算法

Least Recently Used，LRU
选择最久未使用的那个页面淘汰。

对最优页面置换算法的一个近似，根据过去推测未来，这种合理性的依据是程序的局部性原理。

LRU需要记录各个页面使用时间的先后顺序，**开销比较大**。

可能的实现方法有：
+ 可以用一个页面链表，每次访问移动到头上。新增加在头，淘汰最末的。
+ 可以用一个栈，有则抽出重新压入。新的在栈头，淘汰最底的。

### 4.4.5 时钟页面置换算法

对LRU的近似，对FIFO的改进：
Clock页面置换算法

+ 基本思路
利用访问位。对访问时间做一个粗略估计。

用一个循环链表，环形的页面链表。有一个指针，当出现缺页，指针指的页访问位是1，就置0，跳到下一个；为0淘汰掉那个页。

### 4.4.6 二次机会法/Enhance Clock算法

在Clock页面置换算法的基础上，考虑修改位。（used bit 和dirty bit）
减少写回的次数。

在循环的时候，替换掉两个bit都是0的页【00】。对于只有一个bit是1的，置成0【01和10-->00】。对于两个bit是1的，只把used bit置0【11-->01】。

### 4.4.7 最不常用算法

Least Frequently Used，LFU
选择访问次数最少的那个页淘汰。

实现：对每个页设置一个访问计数器，访问+1，淘汰数值最小的。（开销太大）

缺点：
假设可能不合理。如果一个页，初始化时访问次数多，而正常时访问次数少，那么该算法不太合适。
（改进：隔一段时间，减少一下存储的次数，把时间考虑进去）

### 4.4.8 Belady现象

Belady现象（命名是根据科学家的名字）：在用FIFO算法时，有时候分配的物理页越多，缺页率反而提高的异常现象。

原因：FIFO算法与置换算法的目标不一致。并不是置换出最不会访问的页面。

### 4.4.9 LRU，FIFO，Clock的比较

LRU和FIFO：本质都是先进先出，不过LRU考虑到了访问时间。要求程序得有局部性。LRU的性能好，但是开销大；FIFO的开销小，但是性能差；Clock算是两者的折中。
Clock是LRU的近似。

### 4.4.10 工作集模型

局部页面替换算法都是针对一个程序，操作系统实际上是运行着多个程序。

前面介绍的各种页面置换算法，都有一个前提：程序的局部性原理。如果局部性原理不成立，各种置换算法就没区别也没意义了。

可以对局部性原理的定量分析。

+ 工作集：一个进程当前正在使用的逻辑页面集合。
可以用一个二元函数表示$$W(t, \delta)$$
t是当前的执行时刻
$$\delta$$称为工作集窗口（working-set window），即一个定长的页面访问的时间窗口
$$W(t, \delta)$$是时刻t之前$$\delta$$时间窗口中所有页面所组成的集合
$$|W(t, \delta)|$$为集合的大小

局部性比较稳定的时候，工作集大小比较稳定；局部性改变的时候，工作集会快速扩张或收缩。

+ 常驻集：当前时刻，进程实际驻留在内存中的页面集合。

工作集是进程的固有性质，而常驻集取决于OS分配给进程的物理页面数目，以及采用的页面置换算法。

希望两个集合尽量重叠，工作集的内容都在常驻集里。而常驻集的大到一定程度后，再增大不会明显减少缺页率。需要找一个合适的值。
需要用全局置换算法来解决这个问题。

### 4.4.11 工作集页置换算法

+ 基本思想
   随着工作集窗口的变化，已经不属于工作集的页，直接换出。需要的换入。

就算内存还够，也会换出去。
保证整个系统的层次，页缺失率最低。

### 4.4.12 缺页率页面置换算法

page fault frequency， PFF

+ 根据缺页率，来改变常驻集的大小。缺页率高的进程，分配更多的常驻集。

缺页率：缺页次数/内存访问次数
或 缺页的平均时间间隔的倒数

影响缺页率的因素：
+ 页面置换算法
+ 分配给进程的物理页面数目
+ 页面本身的大小
+ 程序的编写方法

+ 具体实现
缺失发生的时候，记录时刻，算一下上次缺页到现在的时间差。如果大于阈值，说明应该减少常驻集，移除掉该时间段内没有被引用的页。如果小于，将缺失页直接加入到常驻集里。

### 4.4.13 缺页率页面置换算法与工作集置换算法的区别

+ 对页的调整时机不一样。PPF只在缺页中断时判断是否更改，而工作集置换算法是在每次换入换出时都做判断。

+ 都与局部页置换算法不一样。这两种涉及到工作集大小的调整。而局部只是在工作集满了之后才考虑换入换出。
+ 全局页置换算法效果比局部页置换算法要好。

### 4.4.11 抖动问题

如果替换页面太频繁，进程的运行速度变得很慢，我们把这个状态称为“抖动”。

量化抖动：
平均页缺失时间MTBF：mean time between page faults
页缺失服务时间PFST：page fault service time

调整让两者相等

+ 不要开太多的程序

# 5 进程

## 5.1 进程的定义

一个具有一定独立功能的程序在一个数据集合上的一次动态执行过程。

### 5.1.1 进程的组成

+ 程序的代码
+ 程序处理的数据
+ 程序计数器的值，指示下一条将运行的指令
+ 一组通用的寄存器的当前值，堆，栈
+ 一组系统资源(如打开的文件)

总之，进程包含了正在运行的一个程序的所有状态信息。

### 5.1.2 进程与程序的联系

+ 程序是产生进程的基础
+ 程序的每次运行构成不同的进程
+ 进程是程序功能的体现
+ 通过多次执行，一个程序可对应多个进程；通过调用关系，一个进程可包括多个程序。

### 5.1.3 进程与程序的区别

+ 进程是动态的，程序是静态的
+ 程序是有序代码的集合；进程是程序的执行
+ 进程有核心态/用户态
+ 进程是暂时的，程序是永久的
+ 进程是一个状态变化的过程，程序可长久保存
+ 进程与程序的组成不同：进程的组成包括程序，数据和进程控制块（进程的状态信息）

### 5.1.4 进程的特点

+ 动态性：可动态地创建，结束进程
+ 并发性：进程可以被独立调度并占用处理机运行
+ 独立性：不同进程的工作不互相影响
+ 制约性：因访问共享数据/资源或进程间同步而产生制约

### 5.1.5 进程控制结构

程序 = 算法 + 数据结构
+ 管理进程的数据结构：进程控制块
+ 进程控制块（process control block, PCB）: 描述进程的数据结构，操作系统管理控制进程运行所用的信息集合。
+ 操作系统为每个进程都维护了一个PCB，用来保存与该进程有关的各种状态信息，PCB是进程存在的唯一标志。

使用PCB，就可以完成进程的创建、终止、组织管理。

**PCB包含下列三大类信息**
1. 进程标识信息。
如本进程的标识，本进程的产生者标识（父进程标识）；用户标识

2. 处理机状态信息保存区。保存进程的运行现场信息（主要是指寄存器）
   + 用户可见寄存器，用户程序可以使用的数据，地址等寄存器
   + 控制和状态寄存器，如程序寄存器(PC)，程序状态字(PSW)
   + 栈指针，过程调用/系统调用/中断处理和返回时需要用到它。

3. 进程的控制信息
   + 调度和状态信息：用于操作系统调度进程并占用处理机使用；
   + 进程间通信信息：为支持进程间的与通信相关的各种标识，信号，信件等，这些信息存在接收方的PCB中；
   + 存储管理信息：包含有指向本进程映像存储空间的数据结构；
   + 进程所用资源：说明由进程打开，使用的系统资源，如打开的文件等；
   + 有关数据结构等连接信息：进程可以连接到一个进程队列中，或连接到相关的其它进程的PCB。

4. PCB的组织方式
   + 链表：统一状态的进程其PCB成一脸表，多个状态对应多个不同的链表，各状态的进程形成不同的链表，例如就绪链表和阻塞链表
   + 索引表：同一状态的进程归入一个index表(由index指向PCB)，多个状态对应多个不同的index，各状态的进程形成不同的索引表，例如就绪索引表，阻塞索引表。

## 5.2 进程状态

### 5.2.1 进程的生命周期管理

+ 进程创建
+ 进程运行
+ 进程等待
+ 进程唤醒
+ 进程结束

#### 5.2.1.1 进程创建

引起进程创建的三个主要事件：
1. 系统初始化
2. 用户请求创建一个新进程
3. 正在运行的进程执行了创建进程的系统调用

#### 5.2.1.2 进程运行

进程创建后，PCB完成初始化，进程就处于就绪状态。
内核选择一个就绪的进程，让它占用处理机并执行。
何时选择，如何选择，涉及到调度。

#### 5.2.1.3 进程等待

在以下情况中，进程等待(阻塞)：
+ 请求并等待系统服务，无法马上完成；
+ 启动某种操作，无法马上完成；
+ 需要的数据没有到达。

进程只能自己阻塞自己，因为只有进程自身才能知道何时需要等待某种事情的发送。

#### 5.2.1.4 进程唤醒

唤醒进程的原因如下：
+ 被阻塞进程需要的资源可被满足；
+ 被阻塞进程等待的事件到达；
+ 将该进程的PCB插入到就绪队列中。

进程只能被别的进程或操作系统唤醒。

#### 5.2.1.5 进程结束

包括以下四种情形：
+ 正常退出(自愿)
+ 错误推出(自愿)
+ 致命错误(强制性)
+ 被其它进程所杀(强制性)

### 5.2.2 进程的状态变化模型

进程的三种基本状态：
进程在生命结束前处于且仅处于三种基本状态之一，不用系统设置的进程状态数目不同。
+ 运行状态(running)：当一个进程正在处理机上运行时；
+ 就绪状态(ready)：一个进程获得了除处理机之外的一切所需资源，一旦得到处理机即可运行；
+ 等待状态(或阻塞状态blocked)：一个进程正在等待某一事件而暂停运行时的状态，如等待资源，等待I/O完成。

进程还有其它的基本状态，包括，
+ 创建状态(new)，一个进程正在被创建，还没被转到就绪状态之前的状态。

+ 结束状态(exit)，一个进程正在从系统中消失时的状态，这是因为进程结束或由于其它原因所导致。

  状态变化图![]()



### 5.2.3 挂起模型

+ 进程挂起suspend
   进程没有占用内存空间，处于挂起状态的进程映像在磁盘上。

+ 进程挂起是一种合理且充分地利用系统资源的方式。挂起就是把一个进程从内存转到外存。

#### 5.2.3.1 挂起状态

+ 阻塞挂起状态（blocked-suspend）：进程在外存并等待某事件的出现
+ 就绪挂起状态（ready-suspend）：进程在外存，但只要进入内存，即可运行

#### 5.2.3.2 与挂起相关的状态转换

##### 5.2.3.2.1 挂起
挂起：就是把一个进程从内存转到外存。可能有以下几种情况：

+ 阻塞-->阻塞挂起：没有进程处于就绪状态；或者就绪进程需要更多的内存资源；
+ 就绪-->就绪挂起：当 高优先级阻塞(系统认为会很快就绪的)进程 和 低优先级就绪进程 冲突时，系统会挂起低优先级就绪进程；
+ 运行-->就绪挂起：对于抢先式分时系统，当有 高优先级阻塞挂起进程 因为事件而变成 就绪挂起 时，系统可能会把正在运行的进程转到 就绪挂起状态。

##### 5.2.3.2.2 外存中

在外存中的状态转换：
+ 阻塞挂起-->就绪挂起：当 阻塞挂起的进程 因为相关事件出现时，系统会把 阻塞挂起进程 转化为 就绪挂起状态。

##### 5.2.3.2.3 解挂

解挂/激活（activate）：把一个进程从外存转到内存；可能有以下几种情况：

+ 就绪挂起-->就绪：现在没有就绪进程；当前的 就绪挂起进程 的优先级高于 就绪进程；
+ 阻塞挂起-->阻塞：当一个进程释放足够的内存时，系统会把一个高优先级的 阻塞挂起进程(系统认为会很快出现所等待的事件发生) 转为阻塞进程。

### 5.2.4 OS如何来管理PCB，完成调度

#### 5.2.4.1 从进程角度看待OS

用进程的观点来看待OS，OS包括各种各样的进程：用户进程，磁盘管理进程，终端进程等；

OS要做的就是：
CPU执行哪个进程？
进程之间的状态转换

#### 5.2.4.2 状态队列

+ 状态队列是由操作系统来维护的一组队列，用来表示系统当中所有进程的当前状态；

+ 不同的状态分别用不同的队列来表示（就绪队列，各种类型的阻塞队列等）；

+ 每个进程的PCB都根据它的状态加入到相应的队列当中，当一个进程的状态发生变化时，它的PCB从一个状态队列中脱离，加入到另一个状态队列里。

## 5.3 线程管理

### 5.3.1 为什么使用线程？

希望多个任务并行进行，但是进程的开销很大，还需要共享数据。

因此，需要满足：
1. 实体间能够并发地执行
2. 实体之间共享相同的地址空间。
这个实体就是线程。

### 5.3.2 线程的定义

线程当中的一条执行流程。

从两个方面重新来理解进程
1. 资源组合的角度
   进程把一组相关的资源组合起来，构成了一个资源平台/环境。
2. 运行的角度
   代码在资源平台上的一条执行流程（线程）。

+ 进程 = 资源管理 + 线程

线程有自己的TCB（Thread Control Block）。

线程的优点：
+ 一个进程中可以同时存在多个线程；
+ 各个线程之间可以并发的执行；
+ 各个线程之间可以共享地址空间和文件等资源。

线程的缺点：
+ 一个线程崩溃，该进程的所有线程崩溃。

不同操作系统对线程的支持不同

线程所需的资源，共同的代码段和数据段，独立的堆栈、寄存器，独立的执行流程

### 5.3.3 线程与进程的比较

+ 线程是资源分配的单位，线程是CPU调度单位；

+ 进程拥有完整的资源平台，而线程只占有必须的资源，如寄存器，栈。

+ 线程同样由就绪，阻塞，执行三种基本状态，同样具有状态之间的转换关系。

+ 线程能减少并发执行的时间和空间开销：
   + 线程的创建时间比进程短
   + 终止时间短
   + 同一进程内切换时间更小（一个需要切换页表，另一个不用）
   + 同一进程内各线程共享内存和文件资源，可直接进行不通过内核的通信

### 5.3.4 线程的实现

有三种线程实现的方法，
**用户线程：**在用户空间实现，OS看不到的线程。由应用程序的库来管理。
**内核线程：**在内核中实现，OS能看到的线程。由OS来管理
**轻量级线程：**在内核中实现，支持用户线程。

用户线程与内核线程的对应关系有：
+ 多对一
+ 一对一
+ 多对多

#### 5.3.4.1 用户线程

+ 在用户空间实现的线程机制，不依赖于操作系统的内核；
+ 由一组用户级的线程库来完成线程的管理，包括创建/终止/同步/调度；

优点：
+ 不需要操作系统内核了解用户线程的存在，可用于不支持线程技术的多进程操作系统；
+ 每个进程都需要它私有的线程控制块TCB列表，来跟踪记录它各个线程的状态信息(PC/栈指针/寄存器),TCB由线程库函数来维护；
+ 用户线程的切换由线程库函数实现，无需 用户态/核心态切换，所以速度快；
+ 允许每个进程有自定义的线程调度算法。

缺点：
+ 如果一个线程发起系统调用而阻塞，则整个进程都在等待；
+ 如果一个线程开始运行，除非它主动交出CPU，否则该线程所在进程的其它线程都无法运行；
+ 由于时间片分配给的是进程，所以与其它进程相比，在多线程执行时，每个线程得到的时间片较少，执行会较慢。

#### 5.3.4.2 内核线程

+ 是指在操作系统的内核中实现的一种线程机制，由操作系统的内核来完成线程的创建，终止和管理。
+ 由内核维护进程和上下文信息，也就是进程/线程控制块PCB/TCB；
+ 线程的创建/终止/切换都是通过系统调用或内核函数来实现(内核实现)，所以系统开销大；
+ 在一个进程中，如果某个内核线程发起系统调用而阻塞，不会影响其它内核线程的运行；
+ 时间片分配给线程，多线程的进程能获得更多的CPU时间；
+ Windows NT/2000/XP 支持内核线程。

#### 5.3.4.3 轻量级进程(lightweight process)

+ 他是内核支持的用户线程。一个进程可以有一个或多个轻量级进程，每个轻量级进程由一个单独的内核线程来支持(Solaris/Linux)。

## 5.4 上下文切换

停止当前运行的进程(从运行态改变成其它状态)并且调度其它进程(转变成运行态)。

切换的是什么？寄存器。

+ 必须在切换之前储存许多部分的进程上下文；
+ 必须能够在之后恢复他们，所以进程不能显示它曾经被暂停过；
+ 必须快速(因为上下文切换非常频繁)。

需要储存上下文：
+ 寄存器(PC/SP/…)，CPU状态，…
+ 一些时候可能会费时，所以需要尽量避免。


+ 操作系统为活跃进程准备了PCB；
+ 操作系统将PCB放置到一个合适的队列里。
   + 就绪队列
   + 等待I/O队列
   + 僵尸队列

### 5.4.1 进程控制

创建进程
加载和执行进程
等待和终止进程

+ 创建进程：Linux系统下的fork/vfork
+ 加载和执行进程：exec（加载程序取代当前运行的进程）
+ 等待和终止进程：wait（父进程等待子进程结束，为了有人回收PLB）

僵尸状态：进程执行完了exit，但是还没有被父进程wait回收，就处于僵尸状态。OS也会定期清理僵尸进程。

# 6 CPU调度

## 6.1 背景知识

+ 上下文切换：
   + 切换CPU的当前任务，从一个进程/线程转换到另一个
   + 保存当前进程/线程在PCB/TCP中的执行上下文(也就是CPU的状态)
   + 读取下一个进程/线程的上下文

+ CPU调度：
   + 从就绪队列中挑选一个进程/线程作为CPU将要运行的下一个进程/线程
   + 调度程序：挑选进程/线程的内核函数（通过一些调度 策略）
   + 什么时候进行调度？

### 6.1.1 在进程/线程生命周期的什么时候进行调度？

从一个状态到另一个状态的时候会发生调度。
尤其是和运行态相关的状态变化。

### 6.1.2 内核运行调度程序的条件(满足其一即可)

1. 一个进程从运行状态切换到等待状态
2. 一个进程被终结了

+ 不可抢占
   + 调度程序必须等待事件结束

+ 可以抢占
   + 调度程序在中断被响应后执行；
   + 当前的进程从运行切换到就绪，或者一个进程从等待切换到就绪；
   + 当前运行的进程可以被换出。

以上指用户态；内核态也可能涉及到是否抢占。
内核态的抢占：在应用程序发起系统调用后，不允许抢占的话，从内核态回到用户态时，一定还是原来的程序；如果允许，就可能变成别的程序。

## 6.2 调度原则

选择进程执行的依据。

### 6.2.1 调度策略

+ 执行模型：程序在CPU突发和I/O中交替
   + 每个调度决定都是关于在下一个CPU突发时将哪个工作交给CPU；
   + 在时间分片机制下，线程可能在结束当前CPU突发前被迫放弃CPU；

### 6.2.1.1 评价指标

+ CPU使用率：CPU处于忙状态所占时间的百分比；
+ 吞吐量：在单位时间内完成的进程数量；
+ 周转时间：一个进程从初始化到结束，包括所有等待时间所花费的时间；
+ 等待时间：进程在就绪队列中的总时间；
+ 响应时间：从一个请求被提交到产生第一次响应所花费的时间。

人们通常都需要“更快”的服务
什么是更快？
+ 传输文件时的**高带宽**
+ 玩游戏时的**低延迟**
+ 这两个因素是独立的

和水管类比
+ 低延迟：喝水的时候想要一打开水龙头水就流出来
+ 高带宽：给泳池充水时希望从水龙头里同时流出大量的水，并不介意是否存在延迟

### 6.2.1.2 希望一个算法有什么效果？

+ 减少响应时间：及时处理用户的输出并尽快将输出提供给用户；
+ 减少平均响应时间的波动：在交互系统中，可预测性比高差异/低平均更重要；
+ 增加吞吐量：减少开支(操作系统开销，上下文切换)；系统资源的高效利用(CPU，I/O设备)；
+ 减少等待时间：减少每个进程的等待时间。

但是以上几点不可能同时实现，所以需要权衡。

低延迟调度增加了交互式表现(个人电脑)：
+ 如果移动了鼠标，希望屏幕迅速反馈光标的移动。

操作系统保证吞吐量不受影响(服务器)：
+ 若想结束长时间的编程，所以操作系统必须不时进行调度，即使存在许多交互任务。


+ 吞吐量是操作系统的计算带宽；
+ 响应时间是操作系统的计算延迟。

### 6.2.1.3 公平性

公平有多种定义

举例：
+ 保证每个进程占用相同的CPU时间
(并不公平，因为一个用户可能比其它用户运行更多进程)；

+ 保证每个进程都有相同的等待时间

公平通常会增加平均响应时间

## 6.3 调度算法

三部分：通常桌面服务器OS、嵌入式实时系统、针对多处理器的

+ FCFS：先来先服务
   + first come，first served
+ SPN（SJF） SRT：短进程优先（短作业优先）短剩余时间优先
   + shortest process next（shortest job first） shortest remaining time；
+ HRRN：最高响应比优先
   + highest response ratio next；
+ Round robin：轮循
   + 使用时间切片和抢占来轮流执行任务
+ Multilevel feedback queues：多级反馈队列
   + 优先级队列中的轮循
+ fair share scheduling,：公平共享调度

### 6.3.1 先来先服务FCFS

FIFO队列规定
+ 如果进程在执行中阻塞，队列中的下一个会得到CPU。

优点：
+ 简单

缺点：
+ **平均等待时间波动**较大（没有考虑抢占）
+ 花费时间少的任务可能排在花费时间长的任务后面
+ 可能导致I/O和CPU之间的重叠处理（CPU密集型进程会导致I/O设备闲置时，I/O密集型进程也在等待）

### 6.3.2 短任务优先SPN

任务执行时间最短的排在前面，可以减少平均等待时间。

+ 按照预测的完成时间排序将任务入列、
+ 可以是抢占的也可以是不抢占的
   + 可抢占：又叫SRT，短剩余时间优先。

优点：
+ 平均等待时间最少

缺点：
+ 可能导致饥饿
   + 连续的短任务流会使长任务饥饿
   + 短任务可用时的任何长任务的CPU时间都会增加平均等待时间
+ 需要预知未来
   + 怎么预估下一个CPU突发的持续时间
   + 简单的解决办法：询问用户
   + 如果用户欺骗就杀死进程
   + 但是用户不知道怎么办？

根据历史时间分配预估未来时间的分配，就是根据之前该进程所花的时间来预测它将要花多少时间。

### 6.3.3 最高响应比优先HRRN

R = (w + s) / s (选择R值最高的进程) 
w：waiting time 等待时间
s：service time 执行时间
R越大，等待时间越长

+ 在SPN调度的基础上改进(考虑了等待时间，改善饥饿现象)
+ 不可抢占
+ 关注进程等待了多长时间
+ 防止无限期推迟
+ 等待时间也需要预估

### 6.3.4 轮循Round robin

各个进程轮流占用CPU执行，注重公平

+ 在叫做**量子(或时间切片)**的离散单元中分配处理器；
+ 时间片结束时，切换到下一个准备好的进程

缺点：
+ 额外的上下文切换
+ 时间量子太大，
   + 等待时间过长
   + 极限情况下退化成FCFS
+ 时间量子太小
   + 反应迅速，但是开销大
   + 吞吐量由于大量的上下文切换开销而受到影响

目标：
   + 选择一个合适的时间量子；
   + 经验规则：维持上下文切换开销处于1%以内。

### 6.3.5 多级反馈队列Multilevel feedback queues

#### 6.3.5.1 多级队列Multilevel queues

想要兼顾SPN和RR的优点：即考虑公平性，又希望能让平均等待时间较短。

将就绪队列划分为独立的不同队列，每个队列有不同的优先级，每个队列可以有自己的调度策略

#### 6.3.5.2 多级反馈队列

任务的优先级可能是会改变的。于是设计反馈队列。

一个进程可以在不同的队列中移动。

例如：时间量子的大小随优先级级别的增加而增加；如果任务在当前的时间量子中没有完成，那么就降到下一个优先级。让交互性比较高的先运行。

对于I/O密集型任务，放在高优先级队列，这里的时间片比较短；
对于CPU密集型任务，放在低优先级队列，这里的时间片比较长。

#### 6.3.6 公平共享调度fair share scheduling 

站在用户的角度实现公平共享CPU资源。因为有的用户可能开的进程多，有的用户进程少。

FFS控制用户对系统资源的访问
+ 一些用户组比其他用户组更重要
+ 保证不重要的组无法垄断资源
+ 未使用的资源按照每个组所分配的资源的比例来分配
+ 没有达到资源使用目标的组获得更高的优先级

### 6.3.7 不同调度模型的评价

+ 确定性建模：确定一个工作量，然后计算每个算法的表现；
+ 队列模型：用来处理随机工作负载的数学方法；
+ 实现/模拟：建立一个允许算法运行实际数据的系统。

### 6.3.8 面向通用计算系统的调度算法总结

实际上这些调度算法和实际的调度算法是有很多区别的（复杂的多），但是基本的特征是类似的。

+ FCFS先来先服务
不公平，等待时间较长

+ SPN/SRT短进程优先
不公平，但是**均等待时间最小**
需要精确预测计算机时间
**可能导致饥饿**

+ HRRN最高响应比优先
给予SPN调度的改进，**考虑了等待时间**
不可抢占

+ Round Robin轮循
**相对公平**，但是平均等待时间较长，**会引入比较多的上下文切换开销**

+ MLFQ多级反馈队列
根据CPU和I/O状态**动态调整**；和SPN类似

+ Fair-share scheduling公平共享调度
**公平**是第一要素，在不同的级别实现公平占用CPU

## 6.4 实时调度

### 6.4.1 实时系统

火车、工业环境的控制程序，要求程序在某一个规定时间内必须完成。real time

+ 定义：正确性依赖于其时间和功能两方面的一种操作系统

+ 性能指标
   + 时间约束的及时性(deadlines)
   + 速度和平均性能相对不重要；

+ 主要特性：
   + 时间约束的可预测性；

实时系统可以分为两类
+ 强实时系统：需要在保证的时间内完成重要的任务，必须完成；
+ 弱实时系统：要求重要的进程的优先级更高，尽量完成，并非必须；

一些术语：
+ 任务（工作单元job）
   + 一次计算，一次文件读取，一次信息传递等
+ 属性
   + 取得进展所需资源
   + 定时参数
released：让任务处于就绪态的时刻
relative deadline：相对的deadline，每个任务和每个任务之间的间隔
absolute deadline：必须完成任务的截止时刻

+ 硬时限：
   + 如果错过了最后期限，可能会发生灾难性或非常严重的后果
   + 必须验证：在最坏的情况下也能够满足时限吗？
   + 保证确定性
+ 软时限：
   + 理想情况下，时限应该被最大满足。如果有时限没有被满足，那么就相应地降低要求
   + 尽最大努力去保证

### 6.4.2 调度算法

实时调度算法有2大类：
1. 静态优先级调度：优先级在任务执行之前确定
2. 动态优先级调度：优先级随着任务执行的过程动态变化

#### 6.4.2.1 RM(Rate Monotonic)速率单调调度

+ 最佳**静态**优先级调度
+ 通过周期安排优先级
+ 周期越短，优先级越高
+ 先执行周期最短的任务

#### 6.4.2.2 EDF(Earliest Deadline First)最早期限调度

+ 最佳的**动态**优先级调度
+ Deadline越早，优先级越高
+ 先执行Deadline最早的任务

## 6.5 多处理器调度

+ 多处理器的CPU调度更加复杂
   + 多个相同的单处理器组成一个多处理器
   + 优点：负载共享
+ 对称多处理器(SMP)
   + 每个处理器运行自己的调度程序
   + 需要在调度程序中同步


主要考虑两个问题：
1. 任务给哪个CPU执行
2. 如何保证负载均衡

## 6.6 优先级反转

低优先级的任务先于高优先级的任务执行了。

### 6.6.1 优先级反转的例子

有3个任务，优先级从高到低为T1，T2，T3。
T3先执行，占用了一块资源；T1执行，也需要这块儿资源，于是T1让出CPU，等待T3执行完成；T3在执行途中，T2执行，抢占了CPU。这时候出现了T2先于T1执行的情况。

+ 优先级反转可以发生在任何基于优先级的可抢占的调度机制
+ 当系统内的环境强制使高优先级任务等待低优先级任务时发生
+ 优先级反转的持续时间取决于其它不相关任务的不可预测的行为

### 6.6.2 解决：优先级继承

当T3占用了T1的资源的时候，让T3继承T1的优先级，这样T2就无法抢占T3的CPU了。

### 6.6.3 解决：优先级天花板

一开始统计所有需要的资源，并对资源定义优先级。
资源的优先级，定义为，要使用这个资源的任务中的最高优先级。

+ 除非当前进程的优先级高于系统中所有被锁定的资源的优先级的上限，否则任务尝试执行临界区的时候会被阻塞；
+ 持有最高优先级上限信号量锁的任务，会继承被该锁所阻塞的任务的优先级。

# 7 同步

## 7.1 背景

多个进程/线程同时工作，可能会访问一些共享资源，如果处理不当，可能会产生一些意想不到的情况。如饥饿、死锁等。

这些问题也和调度有关。

独立的线程：
+ 不和其他线程共享资源或状态
+ 确定性：输入状态决定结果
+ 可重现：能够重现起始条件
+ 调度顺序不重要

合作线程：
+ 在多个线程中共享状态
+ 不确定性
+ 不可重现

因为调度的顺序不同，合作线程会有不确定性、不可重现。会导致一些很难解决的bug，间歇发生，不可重复。

### 7.1 合作的必要性

进程/线程：计算机/设备需要合作

+ 优点1：共享资源
一台电脑，多个用户
一个银行存款余额，多台ATM机
嵌入式系统（机器人控制：手臂和手的协调）

+ 优点2：加速
I/O操作和计算可以重叠
多处理器——将程序分成多个部分并行执行

+ 优点3：模块化
将大程序分解成小程序
使系统易于扩展

### 7.2 结果不确定性的举例

i++;

++这个命令不是原子的。在多线程的情况下会出错。

### 7.3 期望状态

期望能解决掉不确定性的问题：引入同步和互斥。

1. 无论多个线程的指令序列怎样交替执行，程序都必须正常工作
+ 多线程程序具有不确定性和不可重现的特点
+ 不经过专门设计，调试难度很高

2. 不确定性要求并行程序的正确性
+ 先思考清楚问题，把程序的行为设计清楚
+ 切忌急于着手编写代码，碰到问题再调试

## 7.2 一些概念

### 7.2.1 竞态条件

我们把之前说的现象叫做：Race Condition 竞态条件。

+ 系统缺陷：结果依赖于并发执行或事件的顺序： 不确定性，不可重现
+ 避免竞态：让指令不被打断

### 7.2.2 原子操作

原子操作Atomic Operation
+ 原子操作是指一次不存在任何中断或者失败的执行
   + 该执行成功结束
   + 或者根本没有执行
   + 并且不应该发现任何部分执行的状态
+ 实际上操作往往不是原子的
   + 有些操作看上去是原子操作，实际上不是
   + x++这样的简单语句，实际上由3条指令构成
   + 有时候甚至连单挑机器指令都不是原子的

### 7.2.3 临界区

临界区Critical section

临界区是指进程中的一段**需要访问共享资源**并且当另一个进程处于响应代码区域时便不会被执行的**代码**区域。

### 7.2.4 互斥

互斥Mutual exclusion

当一个进程处于临界区并访问共享资源时，没有其他进程会处于临界区并且访问任何相同的共享资源。

### 7.2.5 死锁

死锁Dead Lock

两个或以上的进程，在相互等待完成特定任务，而最终没法将自身任务进行下去

### 7.2.6 饥饿

饥饿Starvation

一个可执行的进程，被调度器持续忽略，以至于虽然处于可执行状态却不被执行。

### 7.2.7 锁

锁Lock

在冰箱上加上保护性装置，使外人无法访问物体内的东西，只能等待解锁后才能访问。（冰箱买面包例子）

### 7.2.8 解锁

解锁Unlock

打开保护性装置，使得可以访问之前被锁保护的物体内的东西。（冰箱买面包例子）

### 7.2.9 死锁

死锁DeadLock

A拿到锁1，B拿到锁2，A想继续拿到锁2后再继续执行，B想继续拿到锁1后再继续执行。导致A和B谁也无法继续执行。（冰箱买面包例子）

## 7.3 临界区

解决竞态条件下的不确定问题：确保临界区内的代码互斥。

**临界区的属性**：

+ 互斥：同一时间临界区中最多存在一个线程
+ Progress前进：如果一个线程想要进入临界区，那么它最终会成功
+ 有限等待：如果一个线程i处于入口区，那么在i的请求被接手之前，其他线程进入临界区的时间是有限制的
+ 无 忙等待（可选）：如果一个进程在等待进入临界区，那么在它可以进入之前会被挂起

## 7.4 实现临界区的3种方法。

### 7.4.1 方法1：禁用硬件中断

+ 没有中断，没有上下文切换，因此没有并发
   + 硬件将中断处理延迟到中断被启用之后
   + 大多数线代计算机体系结构都提供指令来完成
+ 进入临界区
   + 禁用中断
+ 离开临界区
   + 开启中断

缺点：
+ 一旦中断被禁用，线程就无法被停止
   + 整个系统都会为你停下来
   + 可能导致其他线程处于饥饿状态
+ 要是临界区可以任意长怎么办
   + 无法限制响应中断所需的时间（可能存在硬件影响）
+ 要小心使用
+ 对多CPU没有用

### 7.4.2 方法2：基于软件的解决方法

#### 7.4.2.1 Peterson算法：

两个进程

使用2个共享数据项
```
int turn;//指示该谁进入临界区
boolean flag[];//指示进程是否准备好进入临界区，是个数组
```
进入临界区：
```
flag[i]=TRUE;
turn = j;
while(flag[j] && turn==j);
```
退出临界区：
```
flag[i]=FALSE;
```

进程Pi的算法：
```
do{
    flag[i]=TRUE;
    turn = j;
    while(flag[j] && turn==j);
    
    执行临界区代码;
    
    flag[i]=FALSE;
    
    其他代码;
    
}while(TRUE);
```

#### 7.4.2.2 还有一些别的算法：Dekker

#### 7.4.2.3 Bakery算法

n进程

+ 进入临界区之前，进程接收一个数字
+ 得到的数字最小的进入临界区
+ 如果进程Pi和Pj收到相同的数字，那么如果i<j，Pi先进入临界区，否则Pj先进入临界区
+ 编号方案总是按照枚举的增加顺序生成数字

理解：去银行，先去前台取号，按号去办理。如果有两个取号机，人拿到一样的号，看身份证，谁小谁先去。

#### 7.4.2.4 总结

+ 复杂
+ 需要忙等待：浪费CPU时间
+ 需要硬件保证，相对来说比较低，只需要LOAD和STORE指令原子即可。

### 7.4.3 方法3：更高级的抽象

+ 硬件提供了一些原语
+ 操作系统提供更高级的编程抽象来简化并行编程：锁

+ 锁是一个抽象的数据结构：Lock的acquire和release

+ 大多数现代体系结构都提供特殊的原子操作指令

#### 7.4.3.1 Test-and-Set 测试和置位

+ 从内存中读取值
+ 测试该值是否为1（然后返回真或假）
+ 内存值设置为1

即，读取内存boolean，返回该值，并设置内存中为1

#### 7.4.3.2 交换

交换内存中的两个值

#### 7.4.3.3 通过Test-and-Set来实现临界区

Lock初始化：
```
int value = 0;
```
获得锁：
不会进入while循环，并设置value为1；当其他线程到来，会进入while循环忙等
```
Lock：Acquire(){
	while(test-and-set(value)){}
}
```
释放锁：
```
Lock:Release(){
	value=0;
}
```

进程2会在忙等，可以让进程2睡眠。释放锁的时候唤醒该进程。

上下文切换也有开销。所以要根据临界区的长度来权衡。临界区很短就忙等，临界区很长就希望非忙等。

#### 7.4.3.3 通过交换来实现临界区

初始化
```
int lock = 0;
```
线程Ti
```
int key;
do{
    key = 1;
    while(key == 1){
        exchange(lock,key);
    }
    临界区代码
    lock = 0;
    其他代码
}
```

####  7.4.3.4 基于原子操作指令的优缺点

+ 优点
   + 适用于单处理器或者共享主存的多处理器中任意数量的进程
   + 简单并且容易证明
   + 可以用于支持多临界区

+ 缺点
   + 忙等待消耗处理器时间
   + 当进程离开临界区并且多个进程在等待的时候可能导致饥饿
   + 死锁
      如果一个低优先级的进程拥有临界区并且一个高优先级进程也需求，那么高优先级迸程会获得处理器并等待临界区

## 7.5 总结

+ 锁是更高等级的编程抽象
   + 互斥可以使用锁来实现
   + 通常需要一定等级的硬件支持

+ 常用的三种实现方法
   + 禁用中断(仅限于单处理器)
   + 软件方法(复杂)
   + 原子操作指令(单处理器或多处理器均可)

+ 可选的实现内容:
   + 有忙等待
   + 无忙等待

# 8 信号量和管程

## 8.1 信号量

### 8.1.1 为什么需要信号量

lock可以解决互斥问题，但是不能解决同步问题，需要更高级的方式，实现临界区内有多个进程执行。

### 8.1.2 信号量

semaphore

信号量是一种抽象的数据类型
+ 一个整形sem，两个原子操作；
+ P()：sem减1，如果sem<0，等待，否则继续
+ V()：sem加1，如果sem<=0，唤醒一个等待的P

P操作有点像获取锁，起一个阻挡作用。

V：Verhoog，P：Prolaag，分别是荷兰语的增加和减少。
信号量是Dijkstra在20世纪60年代提出。

## 8.2 信号量的使用

### 8.2.1 信号量的属性

+ 信号量是整数，初始值一般是大于0
+ 信号量是一种被保护的变量
   + 初始化后，唯一改变一个信号量值的方法只能是P()和V()
   + 操作必须是原子
+ P()能阻塞，V()不会阻塞
+ 信号量假定是“公平的”
   + 在实践中，FIFO经常被使用

### 8.2.2 两种类型信号量

+ 二进制信号量：0或1
+ 一般/计数信号量：任何非负值
+ 可以用上面这两种类型任意一个实现另一个

### 8.2.3 信号量可以用在2个方面

+ 互斥
+ 条件同步（调度约束——一个线程等待另一个线程的事情发生）

### 8.2.4 使用

互斥：初值为1，进入临界区前P操作，出临界区V操作

同步（调度约束）：初值为0，如果要线程A必须在线程B执行到某个语句后才能执行。线程A先P，等待，线程B执行到某个语句了，执行V，线程A唤醒才能继续执行。

### 8.2.5 有界缓冲区的生产者—消费者问题

+ 一个或多个生产者产生数据并将数据放在缓冲区中；
+ 单个消费者每次从缓冲区取出数据；
+ 在任何一个时间消费者消费的时候，生产者不应该访问缓冲区

**正确性要求**
+ 在任何一个时间只能有一个线程操作缓冲区(互斥)；
+ 当缓冲区为空时，消费者必须等待(调度/同步约束)；
+ 当缓冲区满了时，生产者必须等待(调度/同步约束)；

**实现策略**
一个约束一个信号量：
+ 一个二进制信号量
+ 一个计数信号量fullBuffers
+ 一个计数信号量emptyBuffers

二进制信号量：初始值1
fullbuffers：初始值是0
emptybuffers：初始值是n

生产者：
```
emptyBuffers->P
b->P
add
b->V
fullBuffers->V
```
emptyBuffers初始值是n，每生产一个，减少一个，生产满了就停下来，相当于容量。
b来保证add和remove互斥
最后对fullBuffers进行V操作，表示我现在已经有东西放进去了，可以消费了

消费者：
```
fullBuffers->P
b->P
remove
b->V
emptyBuffers->V
```

注意，下面的两个V操作可以互换顺序，上面两个P操作不能互换顺序

## 8.3 信号量实现

一个整型
一个队列

### 8.3.1 P操作

+ sem减1
+ 如果sem小于0，把线程加到队列里，自身睡眠

### 8.3.2 V操作

+ sem加1
+ 如果sem小于等于0，说明有线程在队列里，出队唤醒

### 8.3.3 注意

+ 信号量的双用途
   + 互斥和条件同步
   + 但等待的条件是独立的互斥

+ 读/开发代码困难

+ 容易出错
   + 使用的信号量被另一个线程占用
   + 忘记释放信号量

+ 不能够处理死锁；

## 8.4 管程

管程最早是语言层面用来处理并发的技术。

### 8.4.1 管程monitor

包含了一系列的共享变量，以及针对这些变量的操作的函数的组合。

+ 一个锁：指定临界区
+ 0个或多个条件变量：等待/通知信号量用于管理并发访问的共享数据（进不去的线程挂到条件变量上）

1. 进入管程是互斥的，有一个进入队列，拿到锁才能进入。
2. 进程进入管程后，执行相应的操作，如果需要等待，则交出锁，睡在一个条件变量上。
3. 条件满足后，唤醒线程，让其有机会继续进行。

### 8.4.2 Lock

Lock::Acquire() ——等待直到锁可用，然后抢占锁
Lock::Release() ——释放锁，唤醒等待者如果有

### 8.4.3 Condition Variable 

+ 允许等待状态进入临界区
允许处于等待(睡眠)的线程进入临界区
某个时刻原子释放锁进入睡眠

+ Wait() operation
释放锁，睡眠，重新获得锁返回后

+ Signal() operation (or broadcast() operation)
唤醒等待者(或者所有等待者)，如果有

### 8.4.4 条件变量的实现

+ 需要维持每个条件队列
+ 线程等待的条件是等待一个signal()操作

Condition的内容：一个整型，一个队列

#### 8.4.4.1 Wait

```
n++;
add;
release(lock);
schedule;//选择下一个线程去执行
require(lock);
```
numWaiting：n
先加，表示条件上挂了一个线程了。然后加到队列里。释放锁。

被唤醒后，选择要执行的线程，获取锁，执行。

#### 8.4.4.2 Signal

```
if(n>0){
	remove
	wakeup(t)
	n--;
}
```
如果条件上没挂线程，就啥都不做。挂了的话，拿出来，唤醒，n减。

### 8.4.5 管程解决生产者—消费者问题

初始化：
```
Lock lock;
int count = 0; //0表示buffer为空，n表示满了
Condition notFull, notEmpty;
```

生产者：
```
lock->require()
while(count==n){
    //buffer满了
    notFull.wait(&lock);
}
add;
count++;
notEmpty.Signal();
lock->release();
```
管程：所以锁要加到头尾


消费者：
```
lock->require()
while(count==n){
    //buffer空了
    notEmpty.wait(&lock);
}
remove;
count--;
notFull.Signal();//Buffer不满了，叫个生产者来生产
lock->release();
```

### 8.4.6 Signal的一个细节

a调用了signal，唤醒了b。是a继续执行，还是b先执行。

a先执行：Hansen

b先执行：Hoare，实现复杂

b先执行的话，上面代码中的while可以改为if。

# 9 经典同步问题

## 9.1 读者—写者问题

动机：
+ 共享数据的访问；

两种类型使用者：
+ 读者：不需要修改数据
+ 写者：读取和修改数据

问题的约束：
+ 允许同一时间有多个读者，但是任何时间只能有一个写者；
+ 当没有写者时，读者才可以访问数据；
+ 当没有读者和其他写者时，写者才能访问数据；
+ 在任何时候只能有一个线程可以操作共享变量。

共享数据：
+ 数据集
+ 信号量CountMutex，初始值为1，实现Rcount的修改互斥
+ 信号量WriteMutex，初始值为1，实现写者的互斥
+ 读者数量Rcount，整数，初始值为1

### 9.1.1 基于信号量的读者优先

读者优先：写者在等待的时候，来了新的读者，新的读者排在写者的前面

写者：
```
P(WriteMutex)

write;

V(WriteMutex)
```

读者：
```
P(CountMutex)
	if(Rcount==0)
		P(WriteMutex)
	Rcount++
V(CountMutex)

read;

P(CountMutex)
	Rcount--
    if(Rcount==0)
        V(WriteMutex)
V(CountMutex)
```

用WriteMutex来控制，读和写之间的互斥。
因为有读者优先，所以加入两个判断：进入的时候，如果Rcount==0，说明没有人在读，那么就尝试拿锁。退出的时候，如果为0，说明自己是最后一个读者，那么就把锁还回去。
所有涉及Rcount的操作用CountMutex来保证原子性。

### 9.1.2 基于信号量的写者优先

读者优先：只要有一个读者处于活动状态，后来的读者都会被接纳。如果读者源源不断的出现，那么写者会始终处于阻塞状态。

写者优先：一旦写者就绪，那么写者会尽可能快的执行写操作。如果写者源源不断的出现，那么读者就始终处于阻塞状态。

课后作业。

### 9.1.3 基于管程的读者优先

有两类写者，一种是正在创作的写者，另一种是在等待队列中的写者。只要有一种写者存在，读者都要等待。

读者：
一直等到没有写者了
读数据
出来，唤醒等待的写者

写者：
等待锁空出来
写数据
监察队列，唤醒

**管程的数据**
```
AR=0 //active readers
AW=0 //active writers
WR=0 //waiting readers
WW=0 //waiting writers
Condition okToRead;
Condition okToWrite;
Lock lock
```

读者：
```
StartRead()//等到没有写者

read;

DoneRead()//唤醒后面的
```
StartRead：
```
lock.acquire()
while((AW+WW)>0){
	WR++
	okToRead.wait(&lock);
	WR--
}
AR++
lock.release()
```
DoneRead：
```
lock.acquire()
AR--
if(AR==0 && WW>0){
	okToWrite.signal()
}
lock.release()
```

写者：
```
StartWrite()//等到没有活动的

write;

DoneWrite()//唤醒
```
StartWrite：
```
lock.acquire()
while((AW+AR)>0){
	WW++;
	okToWrite.wait(&lock);
	WW--
}
AW++
lock.release()
```
DoneWrite：
```
lock.acquire()
AW--;
if(WW>0){
	okToWrite.signal();
}else if(WR>0){
	okToRead.broadcast();//唤醒全部，因为读可以很多个一起
}
lock.release()
```

## 9.2 哲学家就餐问题

5个哲学家围绕一张圆桌而坐。桌子上有5支叉子，每两个哲学家之间放一支。哲学家的动作包括思考和进餐，进餐时需要同时拿起他左边和右边的两支叉子，思考时则同时将两支叉子放回原处。如何保证哲学家们的动作有序进行？如：不出现有人永远拿不到叉子。

### 9.2.1 思路

S1	思考中……
S2	进入饥饿状态
S3	如果左邻居或右邻居正在进餐，进程进入阻塞态；否则转S4
S4	拿起两把叉子
S5	吃……
S6	放下左边的叉子，看看左邻居现在能否进餐（饥饿状态，两把叉子都在），若能则唤醒
S7	放下右边的叉子，看看右邻居现在能否进餐（饥饿状态，两把叉子都在），若能则唤醒
S8	新的一天又开始了，转S1

### 9.2.2 实现：数据结构与信号量

1. 描述每个哲学家的当前状态的数据结构
```
n			5		//哲学家个数
left		i		//第i个哲学家的左邻居
right		(i+1)%n	//第i个哲学家的右邻居
thinking	0		//思考状态
hungry		1		//饥饿状态
eating		2		//进餐状态
state[N]			//记录每个人的状态
```

2. 该状态是一个临界资源，对它的访问应该互斥地进行
```
semaphore mutex			//互斥信号量，初值1
```

3. 一个哲学家吃饱后，可能要唤醒邻居，存在着同步关系
```
semaphore s[N]			//同步信号量，初值0
```

### 9.2.3 实现：函数主体

```
void philosopher(int i){
    while(true){
        think();//S1 思考
        take_forks(i);//S2-S4 拿叉子或被阻塞
        eat();//S5 吃
        put_forks(i);//S6-S7 叉子放回去
    }
}
```

### 9.2.4 实现：拿叉子

take_forks
```
P(mutex)//保护起来所有涉及状态的操作
state[i]=hungry;// 表示饿了
test_take_left_right_forks(i);//试图拿叉子，需要看其他人的状态
V(mutex)
P(s[i]);//没有叉子就阻塞它
```

test_take_left_right_forks(i)
```
if(state[i] == hungry &&
	state[left] != eating &&
	state[right] != eating){
	
	state[i]=eating //两把叉子到手
	V(s[i]) //s[i]的初始值是0，这里就是设为1，设置自身的值，通知一下自己，使得接下来P操作的时候不会被阻塞
}
```

### 9.2.5 实现：放叉子

```
P(mutex) //保护state的变化
state[i] = thinking //放下叉子
test_take_left_right_forks(left)  //看左邻居能行不
test_take_left_right_forks(right)  //看看右邻居
P(mutex)
```

### 9.2.6 实现：think和eat

eat无所谓
think需要更改state为thinking，需要用mutex保护起来

# 10 死锁

## 10.1 死锁问题

### 10.1.1 什么时候出现死锁

以车道为例 

+ 流量只在一个方向。
+ 桥的每个部分可以看作为一个资源。
+ 如果死锁，可能通过一辆车倒退可以解决（抢占资源和回滚）
+ 如果发生死锁，可能几辆车必须都倒退
+ 可能发生饥饿

### 10.1.2 死锁原因

一组阻塞的进程持有一种资源等待获取另一个进程所占有的一个资源。

## 10.2 系统模型

**进程**想要访问**资源**

每个进程可以怎么使用资源：
request/get ——-free resource
use/hold ————requested/used resource, other processes cannot get
release ———–free resource

### 10.2.1 可重复使用的资源

+ 在一个时间只能一个进程使用，且不能被删除。OS避免杀死拥有资源的进程。
+ 进程使用资源后要释放，让其他进程重用
+ 有物理资源（cpu， I/O通道，主和副存储器），也有抽象的资源（设备和数据结构，如文件，数据库和信号量）
+ 如果每个进程拥有一个资源并请求其他资源，可能导致死锁

### 10.2.2 怎么使用资源？

+ 创建，销毁——内存管理单元
+ I/O缓冲区的中断，信号，消息，信息
+ 如果接受信息阻塞可能会发生死锁
+ 少见的组合事件可能会引起死锁

### 10.2.3 资源分配图

一组顶点V和边E的集合

V有两种类型：
   + P={P1, P2…Pn } 所有进程
   + R={R1, R2… Rm} 所有资源类型
+ Requesting/claiming edge –directed edge Pi→Rj 【Pi需要Rj的资源】
+ Assignment/holding edge –directed edge Rj→Pi 【Rj被资源Pi所使用的】

### 10.2.4 通过资源分配图判断死锁

+ 如果图中不包含循环：没有死锁
+ 如果图中包括循环：
   + 如果每个资源类只有一个实例，那么死锁
   + 如果每个资源类有几个实例，可能死锁

## 10.3 死锁特征

死锁可能出现如果四个条件同时成立【必要条件】：

+ 互斥：在一个时间只能有一个进程使用资源
+ 持有并等待：进程保持至少一个资源正在等待获取其他进程持有的额外资源
+ 无抢占：一个资源只能被进程自愿释放
+ 循环等待：存在等待进程集合，一个等待另一个占用的资源，首尾相接（有环）【P0拿P1要的资源，P1拿P2的……Pn拿P0的】。

出现这四个条件不一定出现死锁，但是死锁肯定会出现这四个条件。

## 10.4 死锁处理办法

方法一：确保系统永远不会进入死锁状态

操作系统的功能会被限制，应用系统无法重复的利用cpu执行开销也很大

方法二：运行系统进入死锁状态，然后恢复

但是判断死锁的开销非常大

方法三：忽略这个问题，假装系统中从来没有发生死锁；用于绝大多数的操作系统。

### 10.4.1 死锁预防

思路：只要将之前所说的四个必要条件打破其中的一个，就不会出现死锁。

针对死锁的四个必要条件，打破死锁进行一开始预防：

+ 互斥：共享资源不是必须的，必须占用非共享资源。即本来资源是互斥的，通过使资源不互斥。但是不太好，可能会出现不确定问题。

+ 占用并等待：必须保证当一个进程请求的资源，它不持有其他任何资源。
   + 将条件变大，拿资源就拿全部的资源才去执行，否则不能资源去睡眠，这样就不会存在死锁。但是不同的执行过程中，需要的资源不同，导致一直占用资源但是没有使用，所以会导致系统资源的利用率低。

+ 不抢占：直接将进程kill掉，也就将资源抢占过来了，但是手段暴力，不合理。

+ 循环等待：对所有资源类型进行排序，并要求每个进程按照资源的顺序进行申请。
   + 死锁的出现会出现一个环，打破这个环可以实现死锁的预防。如果对资源类型进行排序，要求进程按资源顺序进行申请，也就是资源只能往上进行申请，这样就不会形成循环的圈。但是前提是要讲资源排好序，但是资源利用还是不合理的。【部分嵌入式使用此方法，因为进程和资源有限。】

### 10.4.2 死锁避免

思路：当进程在申请资源的过程中，然后判断这个申请合不合理，如果会存在死锁的概率，就会拒绝这个请求。

需要系统具有一些额外的先验信息提供：

+ 最简单和最有效的模式是要求每个进程声明它可能需要的每个类型资源的**最大数目**。
+ 资源的分配状态是 通过限定**提供与分配**的资源数量和进程的**最大**需求。
+ 死锁避免算法**动态检查**的资源分配状态，以确保永远不会有一个环形等待状态。

当一个进程请求可用资源，系统必须立即分配是否能使系统处于安全状态。

处于安全状态：针对所有进程，存在安全序列（按照这个序列执行，先后顺序执行，所以的进程都可以正常的等待所需要的资源，正常的结束）。

死锁是不安全状态的一个子集。

死锁避免：确保系统永远不会处于非安全状态。

### 10.4.3 银行家算法

著名的死锁避免算法。

#### 10.4.3.1 背景

客户在第一次申请贷款时，要声明完成该项目所需要的最大资金量，在满足所有贷款要求并完成项目时，客户应该及时归还。
银行家在客户申请的贷款数量不超过自己拥有的最大值时，都应尽量满足客户的需求。

#### 10.4.3.2 前提条件

+ 多个实例
+ 每个进程都必须能最大限度地利用资源
+ 当一个进程请求一个资源，就不得不等待
+ 当一个进程获得所有的资源，就必须在一段有限时间后释放它们

银行家算法会依据上述条件，找到一个安全序列，来决定一个状态是否安全。

#### 10.4.3.3 数据结构

+ n = 进程数量
+ m = 资源类型数量
+ Max ：n×m矩阵，总需求量。Max[i,j]=k，表示进程Pi最多请求资源类型Rj的k个实例
+ Available：长度为m的向量，剩余空闲量。Available[j]=k，表示有k个类型Rj的资源实例可用
+ Allocation：n×m矩阵，已分配量。Allocation[i,j]=k，表示进程Pi当前分配了k个Rj的实例
+ Need：n×m矩阵，未来需要量。Need[i,j]=k，表示进程Pi可能需要至少k个Rj实例完成任务
$$
Need[i,j] = Max[i,j] - Allocation[i,j]
$$

#### 10.4.3.4 安全状态判断算法

1. 初始化：
Work和Finish分别是长度为m和n的向量
```
Work = Available	 //当前资源空闲量
Finish[i] = false    //全部设为false，表示线程i还没结束
```
2. 找这样的i：还没结束，且需要的资源小于等于空闲的资源。找到转3，找不到转4
```
a. Finish[i]=false
b. Need[i]<=work
```
3. 任务可以完成，执行完毕，回收资源
```
Work = Work + Allocation
Finish[i]=true
```
跳到2，再找
4. 找不到这样的i了，如果所有任务都已经结束，安全；否则不安全
```
if Finish[i] == true for all i
then the system is in a safe state.
```
#### 10.4.3.5 银行家算法

1. 如果Request[i]<=Need[i]，转2。否则，提出错误，因为进程已经超过了最大要求
2. 如果Request[i]<=Available，转3。否则Pi等待，资源不够
3. 根据安全状态判断算法判断，如果分配后为安全，将资源分配；如果不安全，Pi等待。

安全判断算法的输入：
$$
Available = Available - Request[i]
Allocation[i] = Allocation[i] + Request[i]
Need[i] = Need[i] - Request[i]
$$

### 10.4.4 死锁检测

+ 允许系统进入unsafe状态，在某一个状态判断当前的系统是否出现死锁，如果是，就启动恢复机制；如果没有，就继续执行，将死锁的检测放在了系统运行中，更往后了。

+ 定期的检测是否有环存在

#### 10.4.4.1 死锁检测算法

1. 等待图：判断是否有环，有环可能死锁
2. 安全状态判断算法

#### 10.4.4.2 缺点

+ 银行家算法开销非常大

+ 定期的执行，操作系统运行开销大

+ 需要一开始就知道max[i,j]

所以在操作系统里几乎不用。
主要用于开发阶段的调试

### 10.4.5 死锁恢复

+ 终止所有的死锁进程
+ 在一个时间内终止一个进程直到死锁消除
+ 终止进程的顺序应该是
   + 进程的优先级
   + 进程运行了多久以及需要多少时间才能完成
   + 进程占用的资源
   + 进程完成需要的资源
   + 多少进程需要被终止
   + 进程是交互还是批处理

都存在某种程度上的强制性和不合理性。所以死锁恢复是最后的手段。

# 11 进程间通信

进程间通信，IPC，Inter Processes Communication

## 11.1 概述

1. 问题：为什么要进行进程间通信？

进程之间可能要完成一个大的任务，这需要一定的数据的沟通和信息的传递。保证进程独立性的同时，保证其可以有效的沟通。

2. IPC提供2个操作

send message
receive message

3. 通信的前提

在他们之间建立通信链路
通过send/receive交换消息

4. 通信链路实现

物理（例如共享内存，硬件总线）
逻辑（例如，逻辑属性）

### 11.1.1 直接/间接通信

+ 间接通信：A和B通过内核来通信
+ 直接通信：A和B访问共享的一块数据来通信

#### 11.1.1.1 直接通信

为了实现直接通信：
+ 进程必须正确地命名对方
   + send( P, message) - 发送信息到进程P
   + receive( Q, message) - 从进程Q接收消息

+ 通信链路的属性
   + 自动建立链路
   + 一条链路恰好对应一对通信进程
   + 每对进程之间只有一个链接存在
   + 链接可以是单向的，但通常为双向的

#### 11.1.1.2 间接通信

实现间接通信，把数据发送到共享区，发送方和接收方都不关注具体的另一方是谁

+ 定向从消息队列接收消息：
   + 每个消息队列都有一个唯一的ID
   + 只有他们共享了一个消息队列，进程才能通信

+ 通信链路的属性：
   + 只有进程共享一个共同的消息队列，才建立链路
   + 链接可以与许多进程相关联
   + 每对进程可以共享多个通信链路
   + 连接可以是单向或者双向

+ 操作：
   + 建立一个新的消息队列
   + 通过消息队列发送和接收消息
   + 销毁消息队列

+ 原语的定义如下：
   + send(A, message) – 发送消息到队列A
   + receive(A, message) - 从队列A接受消息

### 11.1.2 阻塞/非阻塞

消息传递可以是阻塞或非阻塞

+ 阻塞被认为是同步的
发送或接收时，一直要完成才回去做其他事情

+ 非阻塞被认为是异步的
发送或接收的操作，会很快的返回，并不用等到发送完成

### 11.1.3 缓存

利用缓存提高效率，避免发送方和接收方的不匹配问题。

队列的消息被附加到链路的3种方式：
+ 0容量 — 0 messages 发送方必须等待接收方
+ 有限容量 — n messages 的有限长度，发送方在队列满的时候要等待
+ 无限容量 — 理想状况，发送方不用等

## 11.2 信号

Signal

相当于软件级别发出的中断，通知信息。

接收到信号时会发生什么
+ Catch：指定信号处理函数被调用
+ Ignore：什么都不做，依靠操作系统的默认操作
+ Mask： 闭塞信号因此不会传送。可能是暂时的(当处理同样类型的信号)

不足：不能传输要交换的任何数据

### 11.2.1 实现

1. 应用程序先向OS注册对于某个信号的handles
2. 一旦产生信号，OS让当前程序停下，跳到信号处理函数：OS在内核态收到了信号，返回用户态的时候，修改应用程序的堆栈，跳到信号处理函数，把当前程序的下一步地址跟在信号处理函数后面

## 11.3 管道

用于数据交换

将一个文件的输出，重定向到令一个文件的输入，这样就可以完成一系列的操作。

```
$ ls | grep "a"
$ ls | more
```

实现：shell进程收到一条命令之后，会创建两个子进程，ls进程和more进程。同时将ls的输出到一个管道中，内存中的一个buffer。more从管道中接受数据。

这样

就完成了输入输出的重定向功能。这样就完成了分页显示目录的功能。（存在阻塞现象）

+ 两个子进程可以从父进程处继承一些资源
+ 管道是以文件形式存在的

## 11.4 消息队列

用于数据交换

管道：联系是通过父进程建立的，数据是字节流

消息队列：多个不相干的进程可以传递数据。数据可以是一个结构化的数据。

## 11.5 共享内存

管道和消息队列都是间接通信。共享内存是直接通信。

进程
+ 每个进程都有私有地址空间
+ 在每个地址空间内，明确地设置了共享内存段

优点：
+ 快速、方便地共享数据

不足：
+ 必须同步数据访问

### 11.5.1 特点

+ 最快的方法
+ 一个进程写，另一个进程立即可见
+ 没有系统调用干预
+ 没有数据复制
+ 不提供同步：由程序员提供同步

### 11.5.2 实现

不同的逻辑地址，映射到相同的物理地址

## 11.6 Socket

还可以用Socket来通信，这里不讲

# 12 文件系统

## 12.1 用户角度下的文件

## 12.2 文件系统的实现

## 12.3 多磁盘管理

略

https://blog.csdn.net/Y_Mlsy/article/details/107447307