安装Pytorch小坑：在换源后，官网的命令要去掉-c及其后面的部分，不然还是会去官网下载。。。-c那句话就是去官网下载的意思
【MD，换完源后不去-c的话，不搭梯子连不上官网，搭了梯子连不上清华镜像站，折腾了好久】
Pytorch中，torch.Tensor是存储和变换数据的主要工具，有点像NumPy里的多维数组，然而Tensor提供GPU计算和自动求梯度等更多功能，更适合深度学习。
tensor一般翻译为“张量”，可以看作一个多维数组。

创建Tensor：
import torch

5×3的未初始化的Tensor
x = torch.empty(5, 3)
【看起来像是个  5个3维向量？】
随机初始化：
x = torch.rand(5, 3)
long型全0的Tensor
torch.zeros(5, 3, dtype=torch.long)
还可以直接用数据创建
x = torch.tensor([5.5, 3])
还可以用现有的Tensor创建，此方法会默认重用输入的Tensor的一些属性，除非自定义新的属性【如数据类型】
x = x.new_ones(5, 3, dtype=torch.float64)【返回的tensor默认具有相同的torch.dtype和torch.device】
x = torch.randn_like(x, dtype=torch.float)【指定了新的数据类型】
x.size() 或者x.shape获得Tensor的形状【返回torch.Size([5, 3]) ，实际上就是一个tuple】

还有很多函数可以创建Tensor，下面给出一些常用的：

函数 功能
Tensor(*sizes) 基础构造函数
tensor(data,) 类似np.array的构造函数
ones(*sizes) 全1Tensor
zeros(*sizes) 全0Tensor
eye(*sizes) 对角线为1，其他为0
arange(s,e,step) 从s到e，步长为step
linspace(s,e,steps) 从s到e，均匀切分成steps份
rand/randn(*sizes) 均匀/标准分布
normal(mean,std)/uniform(from,to) 正态分布/均匀分布
randperm(m) 随机排列
这些方法都可以在创建的时候指定数据类型dtype和存放device（cpu/gpu）

Tensor操作
算术操作
pytorch中同一种操作会有多种形式，以加法为例：
一：
x + y
二：
torch.add(x, y)
【这种形式还可以指定输出，torch.add(x, y, out=result)【result也是一个Tensor】】
三：inplace，不创建新的对象，直接在原来的对象上进行更改，pytorch操作inplace版本都有后缀_
y.add_(x)
索引【索引的结果与原来的数据共享内存，即修改一个另一个也会修改】
y = x[0, : ]【即第一个三维向量】
pytorch还提供了一些高级的选择函数：
函数 功能
index_select(input, dim, index) 在指定维度dim上选取，比如选取某些行、某些列
masked_select(input, mask) 这个没写清楚吧？没看懂
non_zero(input) 非零元素的下标
gather(input, dim, index) 根据index，在dim维度上选取数据，输出的size与index一样
【这部分没有详细介绍，用到了再查官方文档】
改变形状
view()
【view函数里的参数会出现 -1 ，-1的意思是让电脑自己计算出来应该是多少，因为Tensor的两个值的乘积应该是固定的，就不用自己算】
【注意view()返回的tensor和原来的是同一个，改变一个也会更改另一个，即view只是改变了对这个张量的观察角度】

拷贝：clone()
还有一个函数 reshape()也可以改变形状，但是此函数不能保证返回的是拷贝，不推荐使用

item()【将标量Tensor转换为一个python number】

Pytorch还支持一些线性函数：
函数 功能
trace 求矩阵的迹（对角线元素的和）
diag 对角线元素
triu/tril 矩阵的上三角/下三角，可以指定偏移量
mm/bmm 矩阵乘法，batch的矩阵乘法
addmm/addbmm/addmv/addr/badbmm.. 矩阵运算
t 转置
dot/cross 內积/外积
inverse 求逆矩阵
svd 奇异值分解
【矩阵好久以前学的，好多都只看着眼熟而已，完全不知道都是些啥了】

广播机制
对两个形状不同的Tensor做运算的时候回触发广播机制（broadcasting）：会先复制元素让两个Tensor形状相同后再按元素运算：如1*2和3*1相加，会把各自都复制成3*2然后再相加【[1, 2]会变成 [ [1, 2], [1, 2], [1, 2] ]】

内存开销：索引、view不会开辟新内存，y = x + y会开新内存，然后将y指向新的内存。【+=运算符（即add_()），add函数的out参数，索引，均可以实现将x + y的内存指到原来的y内存】

tensor和numpy互相转换
a.numpy() 【tensor转numpy数组】
from_numpy(a) 【numpy数组转tensor】
可以将tensor与numpy数组互相转换，但是需要注意，这两个函数返回的结果和原来的对象共享相同的内存，改变一个另一个也会变
torch.tensor(a)【numpy数组转tensor】
这个方法总会进行数据拷贝，会比上面两个消耗更多的时间和空间，所以返回的对象与原对象不再共享内存

所有在CPU上的tensor（除了CharTensor）都可以与NumPy数组相互转换

用方法to()可以将Tensor在CPU与GPU之间互相移动（需要硬件支持）
#以下代码只有在PyTorch GPU版本上才会执行
if torch.cuda.is_available():
    device = torch.device("cuda")	#GPU
    y = torch.ones_like(x, device = device)	#直接创建一个在GPU上的Tensor
    x = x.to(device)			#等价于 .to("cuda")
    z = x + y
    print(z)
    print(z.to("cpu", torch.double))	#to()还可以同时更改数据类型

自动求梯度
autograd包能够根据输入和前向传播过程自动构建计算图并进行反向传播
如果将tensor的属性 .requires_grad设置为True，它将开始追踪其上的所有操作。完成计算后可以调用.backward()来完成所有的梯度计算，累积到.grad属性中【在x.backward()的时候，如果x是标量，则函数不需要参数，否则需要传入一个与x同形的tensor】
【最后调用z.backward()，然后会在之前的x.grad里存入z关于x的梯度】
【为什么要传入一个参数呢：是为了避免向量或张量对张量求导，只允许标量对张量求导。如果X是m*n的矩阵，Y是p*q，Z是s*t，然后X经运算得到Y，Y得到Z，那么dZ/dY就是一个s*t*p*q的四维张量，dY/dX就是一个p*q*m*n的四维张量，传播就很麻烦，要四维张量和思维张量乘啊之类的。所以只允许标量对张量求导，求到的结果就和张量同形的张量。传个参数进去y.backward(w)，就会先计算s = torch.sum(y * w)然后s是个标量，然后求s对自变量x的导数】【这里的数学问题呢？】【Sum(y*w)是个加权求和】


如果不想继续追踪，可以调用.detach()把其从追踪记录中分离，此外还可以用with torch.no_grad()把不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用
Function是另一个很重要的类，与Tensor互相结合就可以构建一个记录有整个计算过程的有向无环图（DAG），每一个Tensor都有一个.grad_fn属性，即创建该Tensor的Function。即如果该Tensor是通过某些运算得到的，grad_fn返回一个与这些运算相关的对象，如果不是则返回None。

直接创建的tensor：叶子节点：x.is_leaf   True
.requires_grad默认为False，可以通过a.requires_grad_(True)函数用inplace的方式设置为true

torch.autograd这个包是用来计算一些雅克比矩阵的乘积的【grad在反向传播过程中是累加的，每次运行反向传播梯度都会累加之前的梯度，所以一般在反向传播之前需要把梯度清零】【反向传播说的应该是backward】【累加应该是指，如果有一个out2和out，out计算完后的梯度存在了x.grad里，然后out2再计算，也会存在x.grad里，但是不是替换值，而是加上去】

如果想修改tensor的数值又不希望被autograd记录（即不会影响反向传播），那么可以对tensor.data操作
x.data也是一个tensor， x.data *= 100 这个操作会改变值但是不会记录在计算图，不会影响梯度传播，会更变tensor的值
